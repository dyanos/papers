{
  "version": 2,
  "references": [
    {
      "citeKey": "weixun2025",
      "url": "https://arxiv.org/abs/2512.24873",
      "type": "academic",
      "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
      "authors": [
        "Wang",
        "Weixun",
        "Xu",
        "XiaoXiao",
        "An",
        "Wanhe",
        "Dai",
        "Fangwen",
        "Gao",
        "Wei",
        "He",
        "Yancheng",
        "Huang",
        "Ju",
        "Ji",
        "Qiang",
        "Jin",
        "Hanqi",
        "Li",
        "Xiaoyang",
        "Li",
        "Yang",
        "Li",
        "Zhongwen",
        "Lin",
        "Shirong",
        "Liu",
        "Jiashun",
        "Liu",
        "Zenan",
        "Luo",
        "Tao",
        "Muhtar",
        "Dilxat",
        "Qu",
        "Yuanbin",
        "Shi",
        "Jiaqiang",
        "Sun",
        "Qinghui",
        "Tan",
        "Yingshui",
        "Tang",
        "Hao",
        "Wang",
        "Runze",
        "Wang",
        "Yi",
        "Wang",
        "Zhaoguo",
        "Wu",
        "Yanan",
        "Xiong",
        "Shaopan",
        "Xu",
        "Binchen",
        "Xu",
        "Xander",
        "Xu",
        "Yuchi",
        "Zhang",
        "Qipeng",
        "Zhang",
        "Xixia",
        "Zhao",
        "Haizhou",
        "Zhao",
        "Jie",
        "Zhao",
        "Shuaibing",
        "Zheng",
        "Baihui",
        "Zheng",
        "Jianhui",
        "Zheng",
        "Suhang",
        "Zhu",
        "Yanni",
        "Cai",
        "Mengze",
        "Cao",
        "Kerui",
        "Chen",
        "Xitong",
        "Dai",
        "Yue",
        "Du",
        "Lifan",
        "Feng",
        "Tao",
        "He",
        "Tao",
        "Hu",
        "Jin",
        "Hu",
        "Yijie",
        "Jiang",
        "Ziyu",
        "Li",
        "Cheng",
        "Li",
        "Xiang",
        "Liang",
        "Jing",
        "Lin",
        "Xin",
        "Liu",
        "Chonghuan",
        "Liu",
        "ZhenDong",
        "Lv",
        "Zhiqiang",
        "Mi",
        "Haodong",
        "Mo",
        "Yanhu",
        "Ni",
        "Junjia",
        "Pei",
        "Shixin",
        "Shen",
        "Jingyu",
        "Song",
        "XiaoShuai",
        "Wang",
        "Cecilia",
        "Wang",
        "Chaofan",
        "Wang",
        "Kangyu",
        "Wang",
        "Pei",
        "Wang",
        "Tao",
        "Wang",
        "Wei",
        "Xiao",
        "Ke",
        "Xu",
        "Mingyu",
        "Xu",
        "Tiange",
        "Ya",
        "Nan",
        "Yang",
        "Siran",
        "Ye",
        "Jianan",
        "Zang",
        "Yaxing",
        "Zhang",
        "Duo",
        "Zhang",
        "Junbo",
        "Zheng",
        "Boren",
        "Deng",
        "Wanxi",
        "Pan",
        "Ling",
        "Qu",
        "Lin",
        "Su",
        "Wenbo",
        "Wang",
        "Jiamang",
        "Wang",
        "Wei",
        "Wei",
        "Hu",
        "Wu",
        "Minggang",
        "Yu",
        "Cheng",
        "Zhao",
        "Bing",
        "Zheng",
        "Zhicheng",
        "Zheng",
        "Bo"
      ],
      "date": "2025/12/31",
      "description": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agentic model. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME, an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-Perceptive Agentic Policy Optimization (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of ALE.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "1\\. 문제: agentic crafting은 한 번 답하고 끝이 아니라, 여러 턴 동안 tool을 쓰고 결과를 보고 산출물을 계속 고치며 목표를 만족해야 해서, 데이터 생성·학습·실행 인프라가 약하면 연구가 진행되기 어렵고 파이프라인 병목에서 막힌다고 봄. \n\n2\\. Agent 학습용 풀스택으로 제안함: ROCK이 샌드박스 환경을 관리하며 대규모 trajectory를 만들고, ROLL이 그 trajectory로 post-training 최적화(RL/SFT 등)를 돌리며, iFlow CLI가 실제 실행 시 컨텍스트 구성·툴 라우팅·로그 수집 같은 “에이전트 운영 레이어”를 담당하는 식으로 역할을 분리함. \n\n3\\. ROME(ROME is Obviously an Agentic Model)이라는 모델을 학습한 결과물로 제시하며, 100만+ trajectory로 훈련했다고 밝힘. 여기서 요지는 모델만이 아니라 “환경에서 상호작용→로그/검증→학습으로 재투입”되는 루프가 표준화돼야 성능이 스케일된다는 주장임. \n\n4\\. 데이터는 단순한 정적 데모만 모으는 게 아니라, 멀티턴에서 자주 생기는 실패(잘못된 툴 호출, 중간 산출물 품질 저하, 되돌리기/재시도 등)를 포함해 “복잡 행동”이 나오도록 구성하는 data composition 프로토콜을 강조함. 즉 성공 궤적만이 아니라 실패와 회복까지 학습 신호로 쓰는 쪽임. \n\n5\\. IPA(Interaction-Perceptive Agentic Policy Optimization)의 직관은 토큰 단위로 전부 같은 크레딧을 주지 말고, 실제 에이전트 관점에서 의미 있는 단위(예: 검색→결과 읽기→클릭 같은 한 덩어리 상호작용)를 semantic interaction chunk로 묶어서 그 덩어리 단위로 credit assignment를 하자는 것임. 긴 horizon에서 “어느 턴/행동이 이득이었는지”가 토큰 레벨보다 덜 흐려져 학습이 안정화된다고 설명함. \n\n6\\. 이 접근은 멀티턴 에이전트에서 흔한 문제(보상이 끝에만 있고, 중간에는 noisy한 관측/툴 출력이 많아서 토큰 레벨 학습이 흔들리는 문제)를 “상호작용 이벤트 중심”으로 재정렬해 주는 효과를 노림. 그래서 iFlow/ROCK에서 기록되는 로그가 곧 chunk 경계와 정렬된 학습 데이터가 되게 설계되는 그림임. \n\n7\\. 평가 쪽에서는 Terminal Bench Pro를 소개하면서 스케일과 contamination control을 개선했다고 주장하고, ROME을 SWE-bench Verified, Terminal Bench 계열 등에서 평가해 ALE+IPA 스택이 성능에 기여한다고 결론냄. \n\n8\\. 공개된 모델 카드 요약 기준으로, ROME은 (MoE 30B, active 3B 설정이라고 소개되며) Terminal-Bench 2.0 24.72%, SWE-bench Verified 57.40%를 보고함.",
      "id": "7fd07709-732f-476a-b978-b1cb9121ba32",
      "createdAt": "2026-02-13T11:57:54.989Z",
      "updatedAt": "2026-02-13T11:58:22.051Z"
    },
    {
      "citeKey": "zhaoyang2026",
      "url": "https://arxiv.org/abs/2602.10090",
      "type": "academic",
      "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
      "authors": [
        "Wang",
        "Zhaoyang",
        "Xu",
        "Canwen",
        "Liu",
        "Boyi",
        "Wang",
        "Yite",
        "Han",
        "Siwei",
        "Yao",
        "Zhewei",
        "Yao",
        "Huaxiu",
        "He",
        "Yuxiong"
      ],
      "date": "2026/02/10",
      "description": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [
        "Agent",
        "World Model"
      ],
      "collections": [],
      "notes": "1\\. 문제: agent RL을 크게 키우려면 환경이 많이 필요하지만, 현실 환경은 느리고 비싸며, LLM로 “말로만” 시뮬레이션한 환경은 상태 전이가 들쭉날쭉해서 학습 신호가 불안정하다는 점에서 출발함.\n\n2\\. 제안: Agent World Model(AWM)은 환경을 “텍스트 역할극”이 아니라 코드+DB로 합성하는 파이프라인임. 1,000개 일상 시나리오 환경을 만들고, 평균 35개 도구를 가진 tool-use 환경으로 노출함. 상태는 SQLite DB가 들고, 행동은 API 호출, 관측은 API 응답으로 정의됨.\n\n3\\. 환경을 만드는 절차(직관): 시나리오 하나를 정하면 그 시나리오에서 유저가 할 법한 “해야 할 일(task)” 10개를 먼저 만들고, 그 일을 가능하게 하는 최소한의 DB 스키마/초기데이터를 만든 뒤, 그 DB를 조작하는 도구(API)들을 생성해서 “실제로 실행되는 서비스”처럼 굴리게 함.\n\n4\\. 구체 파이프라인(생성물 기준): (scenario) → (tasks) → (DB schema+sample data) → (API spec) → (환경 서버 코드: FastAPI + MCP 인터페이스) → (검증 코드 verifiers) 순으로 뽑음. 이때 verifiers는 “DB 상태가 목표대로 바뀌었는지”를 코드로 검사해서 reward를 안정적으로 주기 위한 장치임.\n\n5\\. 학습이 쉬워지는 포인트: 환경이 완전 실행 가능(executable)하고 내부 DB state를 접근 가능하게 설계돼서, “LLM-judge 말판정” 대신 “DB 상태 검사”로 보상을 만들 수 있음. 그래서 reward hacking/판정 불안정이 줄고, rollout도 현실 환경보다 훨씬 싸게 많이 찍을 수 있다는 논리임.\n\n6\\. 실험 구성: 벤치마크 전용 환경에서만 훈련하는 대신, AWM으로 만든 합성 환경에서만 RL로 훈련한 뒤, 서로 다른 3개 벤치마크로 OOD 일반화를 평가하는 설정을 강조함.\n\n7\\. 성능(요약 수치): 공개 요약 기준으로 BFCLv3에서 8B 모델이 53.83→65.94(+12.11)로 상승, MCP-Universe에서 8B가 6.70→11.17로 상승, τ²-bench에서는 14B가 Pass@1 39.03에 도달했다고 보고됨.\n\n8\\. 결과가 말하는 바: “벤치마크 맞춤 환경”이 아니라 “일반 합성 환경 묶음”에서 RL을 돌려도, 오히려 도구사용/멀티턴 상호작용 능력이 범용적으로 올라가서 OOD 벤치에서 이득이 난다는 메시지임.",
      "id": "91320af7-31f1-49cb-af74-7dfe161070b8",
      "createdAt": "2026-02-13T11:59:46.977Z",
      "updatedAt": "2026-02-13T11:59:59.971Z"
    },
    {
      "citeKey": "yujiong2026",
      "url": "https://arxiv.org/abs/2602.12984",
      "type": "academic",
      "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
      "authors": [
        "Shen",
        "Yujiong",
        "Yang",
        "Yajie",
        "Xi",
        "Zhiheng",
        "Hu",
        "Binze",
        "Sha",
        "Huayu",
        "Zhang",
        "Jiazheng",
        "Peng",
        "Qiyuan",
        "Shang",
        "Junlin",
        "Huang",
        "Jixuan",
        "Fan",
        "Yutao",
        "Tong",
        "Jingqi",
        "Dou",
        "Shihan",
        "Zhang",
        "Ming",
        "Bai",
        "Lei",
        "Yin",
        "Zhenfei",
        "Gui",
        "Tao",
        "Ma",
        "Xingjun",
        "Zhang",
        "Qi",
        "Huang",
        "Xuanjing",
        "Jiang",
        "Yu-Gang"
      ],
      "date": "2026/02/13",
      "description": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [
        "Gym",
        "Scientic Agent"
      ],
      "collections": [],
      "notes": "이 논문은 **SciAgentGym**이라는 새로운 벤치마크 환경을 소개합니다. 핵심 요점은 다음과 같습니다:\n\n### 주요 내용 요약\n\n- **문제의식**: 기존 과학 벤치마크는 단순한 질의응답 중심이라 실제 과학 연구에서 필요한 **도구 활용 기반의 다단계 추론**을 반영하지 못함.\n\n- **SciAgentGym**: 물리학, 화학, 생물학, 재료과학 등 4개 분야에서 **1,780개 도메인 특화 도구**를 통합한 대규모 상호작용 환경. 파일 시스템, 데이터베이스, 파이썬 인터프리터 등 실행 인프라 포함.\n\n- **SciAgentBench**: 259개 과제와 1,134개 하위 질문으로 구성된 평가 세트. 난이도를 L1(단순)\\~L3(장기적 복잡)로 구분해 모델의 도구 활용 능력을 정밀하게 측정.\n\n- **주요 발견**: 최신 모델(GPT-5 포함)도 **복잡한 장기적 과학 워크플로우**에서는 성능이 급격히 저하됨. 예: GPT-5는 단순 과제(L1)에서 60.6% 성공률이지만, 복잡 과제(L3)에서는 30.9%로 하락.\n\n- **SciForge**: 도구 간 의존성을 그래프 구조로 모델링해 **논리적 실행 경로 기반 학습 데이터**를 합성하는 방법. 이를 통해 모델이 오류 복구와 다단계 추론을 더 잘 수행하도록 개선.\n\n- **성과**: SciAgent-8B 모델은 훨씬 큰 모델(Qwen3-VL-235B-Instruct)보다 뛰어난 성능을 보였으며, **분야 간 전이 학습 효과**도 확인됨.\n\n- **결론**: 도구 활용 기반의 학습과 평가가 과학적 문제 해결에 필수적이며, SciAgentGym과 SciForge는 차세대 과학 AI 에이전트 개발의 토대를 마련함.",
      "id": "115d3924-ab41-41d1-99b0-e1150f0662f9",
      "createdAt": "2026-02-17T11:17:23.445Z",
      "updatedAt": "2026-02-17T11:20:59.761Z"
    },
    {
      "citeKey": "yuzheng2025",
      "url": "https://arxiv.org/abs/2510.08191v1",
      "type": "academic",
      "title": "Training-Free Group Relative Policy Optimization",
      "authors": [
        "Cai",
        "Yuzheng",
        "Cai",
        "Siqi",
        "Shi",
        "Yuchen",
        "Xu",
        "Zihan",
        "Chen",
        "Lichao",
        "Qin",
        "Yulei",
        "Tan",
        "Xiaoyu",
        "Li",
        "Gang",
        "Li",
        "Zongyi",
        "Lin",
        "Haojia",
        "Mao",
        "Yong",
        "Li",
        "Ke",
        "Sun",
        "Xing"
      ],
      "date": "2025/10/09",
      "description": "Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [
        "GRPO"
      ],
      "collections": [],
      "notes": "[\\[논문 리뷰\\] Training-Free Group Relative Policy Optimization | SuanLab Blog | SuanLab](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/)\n\nLLM의 성능을 향상시키기 위한 기존 연구는 주로 강화 학습과 지도 학습을 결합한 방법론에 초점을 맞추고 있습니다. 대표적인 연구로는 다음과 같은 것들이 있습니다:\n\n1. **Supervised Fine-Tuning (SFT)**: LLM의 성능을 개선하기 위해 대규모의 레이블링된 데이터셋을 사용하여 모델을 미세 조정하는 방법입니다. 그러나 이는 데이터가 많이 필요하고, 과적합의 위험이 있습니다. 최근에는 LoRA(Low-Rank Adaptation)와 같은 파라미터 효율적인 파인튜닝(PEFT) 기법들이 SFT의 단점을 보완하기 위해 연구되고 있습니다.\n\n2. **Reinforcement Learning with Human Feedback (RLHF)**: 인간의 피드백을 활용하여 LLM의 출력을 향상시키는 방법입니다. 이는 인간의 주관적인 평가를 반영할 수 있지만, 피드백 수집에 많은 비용과 시간이 소요됩니다. DPO(Direct Preference Optimization)와 같은 알고리즘은 RLHF의 복잡성을 줄이고 안정성을 높이는 방향으로 발전하고 있습니다.\n\n3. **Group Relative Policy Optimization (GRPO)**: 강화 학습에서 사용되는 정책 최적화 방법 중 하나로, 그룹 내에서 상대적인 성능을 비교하여 정책을 개선합니다. 그러나 이는 여전히 매개변수 업데이트가 필요합니다. 기존 GRPO는 정책 경사(policy gradient) 방법을 사용하여 정책을 업데이트하지만, Training-Free GRPO는 이러한 업데이트 과정을 생략합니다.\n\n4. **Experience Replay**: 과거의 경험을 저장하고 재사용하여 학습 효율성을 높이는 방법입니다. 이는 데이터 활용도를 높일 수 있지만, LLM의 경우 경험을 저장하고 재사용하는 것이 쉽지 않습니다. LLM의 경우, 경험을 저장하는 대신 프롬프트 엔지니어링을 통해 유사한 효과를 얻을 수 있습니다.\n\n5. **Prompt Engineering**: LLM의 성능을 향상시키기 위해 프롬프트를 조정하는 방법입니다. 이는 모델의 출력에 큰 영향을 미칠 수 있지만, 최적의 프롬프트를 찾는 것이 어렵습니다. AutoPrompt와 같은 자동 프롬프트 생성 기법은 최적의 프롬프트를 찾는 과정을 자동화하려는 시도입니다.\n\nTraining-Free GRPO는 이러한 기존 방법론과 차별화됩니다. 이 방법론은 매개변수 업데이트 없이도 성능을 향상시킬 수 있으며, 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정합니다. 이는 데이터 부족 문제를 해결하고, 과적합을 방지할 수 있는 장점이 있습니다. 또한, 기존 방법들이 파인튜닝 과정에서 발생할 수 있는 catastrophic forgetting 문제를 완화할 수 있습니다.\n\n**연구 방법론특징차별점**SFT대규모 레이블링 데이터 필요데이터 효율성 부족, 과적합 위험RLHF인간 피드백 활용비용과 시간 소요, 주관성 개입GRPO그룹 내 상대적 성능 비교매개변수 업데이트 필요, 연산 비용 증가Experience Replay경험 저장 및 재사용LLM 적용 어려움, 메모리 부담Prompt Engineering프롬프트 조정최적 프롬프트 찾기 어려움, 휴리스틱 의존**Training-Free GRPO**파라미터 업데이트 불필요, 토큰 사전 학습데이터 효율성 높음, 과적합 방지, 연산 비용 절감\n\n## [**핵심 기여**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%B5%EC%8B%AC-%EA%B8%B0%EC%97%AC)\n\n1. **Training-Free GRPO 제안**: 매개변수 업데이트 없이 LLM의 성능을 향상시키는 새로운 방법론을 제안합니다. 이는 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정합니다. 이는 LLM의 implicit knowledge를 효과적으로 활용하는 새로운 접근 방식입니다.\n\n2. **데이터 효율성 개선**: 소수의 훈련 샘플만으로도 성능을 향상시킬 수 있는 데이터 효율적인 방법을 제시합니다. 이는 데이터 부족 문제를 해결하는 데 기여합니다. 특히 few-shot 또는 zero-shot learning 환경에서 유용합니다.\n\n3. **다양한 도메인에서의 성능 향상**: 수학적 추론, 웹 검색 등 다양한 작업에서 효과적인 성능 향상을 보였습니다. 이는 LLM 에이전트의 활용 가능성을 넓히는 데 기여합니다. 이는 Training-Free GRPO가 특정 도메인에 국한되지 않고 일반적인 문제 해결 능력 향상에 기여함을 시사합니다.\n\n4. **과적합 방지**: 경험적 지식을 활용하여 과적합을 방지할 수 있는 방법을 제시합니다. 이는 모델의 일반화 성능을 향상시키는 데 기여합니다. 토큰 사전은 일종의 regularization 효과를 제공하여 과적합을 방지합니다.\n\n## [**제안 방법론**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%A0%9C%EC%95%88-%EB%B0%A9%EB%B2%95%EB%A1%A0)\n\nTraining-Free GRPO는 매개변수 업데이트 없이도 LLM의 성능을 향상시키는 방법론입니다. 이 방법론의 핵심 아이디어는 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정하는 것입니다. 이는 데이터 부족 문제를 해결하고, 과적합을 방지할 수 있는 장점이 있습니다. Training-Free GRPO는 LLM의 출력 분포를 미세하게 조정하여 정답에 가까운 출력을 유도합니다.\n\n### [**모델 아키텍처**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EB%AA%A8%EB%8D%B8-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98)\n\nTraining-Free GRPO는 LLM의 출력 분포를 조정하기 위해 경험적 지식을 토큰 사전으로 학습합니다. 이 토큰 사전은 긍정적인 피드백을 받은 롤아웃에서 추출한 토큰 시퀀스로 구성됩니다. 이를 통해 LLM의 출력을 조정하여 더 적절한 응답을 생성할 수 있습니다. 토큰 사전은 LLM이 생성할 가능성이 낮은 토큰 시퀀스를 강조하여 탐색 공간을 효율적으로 탐색하도록 돕습니다.\n\n### [**핵심 수식**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%B5%EC%8B%AC-%EC%88%98%EC%8B%9D)\n\n1. **그룹 생성**: LLM 에이전트로부터 다양한 응답 샘플 (롤아웃)을 생성합니다. Rollouts={LLM(Inputi)∣i=1,2,…,n}Rollouts={LLM(Input*i*​)∣*i*=1,2,…,*n*} 여기서 InputiInput*i*​는 프롬프트이며, LLM(Inputi)LLM(Input*i*​)는 LLM이 생성한 응답입니다. 다양한 롤아웃을 생성하기 위해 temperature sampling 또는 top-p sampling과 같은 방법을 사용할 수 있습니다.\n\n2. **상대 평가**: 그룹 내 롤아웃들을 비교하여 상대적인 의미론적 이점을 평가합니다. Semantic Advantage=Evaluate(Rollouts)Semantic Advantage=Evaluate(Rollouts) EvaluateEvaluate 함수는 롤아웃의 품질을 평가하는 함수입니다. 이 함수는 휴리스틱 기반으로 설계될 수도 있고, 별도의 평가 모델을 사용할 수도 있습니다. 예를 들어, 수학적 추론 문제에서는 정답 여부를 평가할 수 있고, 웹 검색 작업에서는 관련성 점수를 평가할 수 있습니다.\n\n3. **지식 증류**: 상대 평가 결과를 바탕으로, LLM의 행동을 유도하는 토큰 사전을 생성합니다. Token Dictionary=Extract(Semantic Advantage)Token Dictionary=Extract(Semantic Advantage) ExtractExtract 함수는 높은 semantic advantage를 가진 롤아웃에서 유용한 토큰 시퀀스를 추출하는 함수입니다. 예를 들어, 가장 높은 점수를 받은 롤아웃에서 자주 등장하는 토큰들을 추출할 수 있습니다. 이러한 토큰들은 LLM이 올바른 방향으로 나아가도록 돕는 역할을 합니다.\n\n4. **추론 시점 적용**: 새로운 입력이 주어졌을 때, 토큰 사전을 사용하여 LLM의 출력을 조정합니다. Output=LLM(Input+Token Dictionary)Output=LLM(Input+Token Dictionary) 토큰 사전은 프롬프트에 추가되어 LLM의 출력을 유도합니다. 이때, 토큰 사전의 위치나 형식을 조정하여 성능을 최적화할 수 있습니다. 예를 들어, 토큰 사전을 프롬프트의 시작 부분에 추가하거나, 특정 토큰 사이에 삽입할 수 있습니다.\n\n5. **정책 최적화 효과**: 각 최적화 단계에서, 그룹 내 롤아웃을 비교하여 의미론적 이점을 도출하고, 이를 통해 정책 최적화 효과를 달성합니다. Optimized Policy=Optimize(Semantic Advantage)Optimized Policy=Optimize(Semantic Advantage) 이 수식은 Training-Free GRPO가 명시적인 정책 업데이트 없이도 정책 최적화 효과를 달성함을 나타냅니다. 각 단계에서 LLM은 더 나은 응답을 생성하도록 유도되며, 이는 마치 정책이 업데이트되는 것과 같은 효과를 냅니다.\n\n**코드 설명 및 개선 사항:**\n\n- `model_name = \"gpt2\"`: 더 작은 모델인 `gpt2`를 사용하여 코드 실행 가능성을 높였습니다. (GPT-2는 비교적 적은 리소스로 실행 가능)\n- `temperature=0.7`: `model.generate` 함수에 `temperature` 파라미터를 추가하여 롤아웃의 다양성을 확보했습니다.\n- `evaluate_rollouts` 함수: 정답(\"Paris\") 포함 여부를 기준으로 롤아웃을 평가하는 간단한 예시를 추가했습니다.\n- `extract_token_dictionary` 함수: 가장 높은 점수를 받은 롤아웃에서 토큰을 추출하는 예시를 추가했습니다.\n- 주석 추가: 코드의 각 부분에 대한 설명을 추가하여 이해도를 높였습니다.\n\n**주의:** 위 코드는 예시이며, 실제 사용 시에는 `evaluate_rollouts` 및 `extract_token_dictionary` 함수를 문제에 맞게 적절히 구현해야 합니다.\n\n## [**실험 설정**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%8B%A4%ED%97%98-%EC%84%A4%EC%A0%95)\n\n실험은 수학적 추론과 웹 검색 작업에서 Training-Free GRPO의 성능을 평가하기 위해 설정되었습니다. 사용된 데이터셋은 공개적으로 사용 가능한 수학 문제 데이터셋(예: MATH dataset)과 웹 검색 쿼리 데이터셋입니다. 평가 지표로는 정확도, 정밀도, 재현율 등이 사용되었습니다. 베이스라인으로는 기존의 소규모 LLM 미세 조정 방법이 사용되었습니다. 특히, few-shot learning 환경에서 Training-Free GRPO의 성능을 집중적으로 평가했습니다.\n\n### [**하이퍼파라미터 표**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%91%9C)\n\n**하이퍼파라미터값설명**학습률N/ATraining-Free 방법이므로 해당 없음배치 크기16롤아웃 생성 시 배치 크기에포크 수N/ATraining-Free 방법이므로 해당 없음최대 시퀀스 길이512LLM의 최대 입력 시퀀스 길이토큰 사전 크기100토큰 사전의 최대 토큰 수롤아웃 수5각 입력에 대해 생성하는 롤아웃 수Temperature0.7롤아웃 생성 시 temperature 값\n\n## [**실험 결과 분석**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%8B%A4%ED%97%98-%EA%B2%B0%EA%B3%BC-%EB%B6%84%EC%84%9D)\n\nTraining-Free GRPO를 적용한 결과, DeepSeek-V3.1-Terminus 모델의 성능이 크게 향상되었습니다. 특히, 수학적 추론과 웹 검색 작업에서 기존의 소규모 LLM 미세 조정 방법보다 더 나은 성능을 보였습니다. 이는 Training-Free GRPO가 LLM의 기존 지식을 효과적으로 활용하여 성능을 향상시킴을 의미합니다.\n\n### [**주요 결과 표**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%A3%BC%EC%9A%94-%EA%B2%B0%EA%B3%BC-%ED%91%9C)\n\n**작업베이스라인 정확도Training-Free GRPO 정확도성능 향상률 (%**)수학적 추론75%85%13.3%웹 검색70%82%17.1%\n\n### [**Ablation Study 분석**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#ablation-study-%EB%B6%84%EC%84%9D)\n\nAblation Study 결과, 토큰 사전의 크기와 상대 평가 방법이 성능에 큰 영향을 미치는 것으로 나타났습니다. 특히, 토큰 사전의 크기가 증가할수록 성능이 향상되었으며, 상대 평가 방법의 정교함이 성능 향상에 기여했습니다. 이는 토큰 사전이 LLM의 출력을 효과적으로 유도하고, 상대 평가 방법이 롤아웃의 품질을 정확하게 평가하는 것이 중요함을 시사합니다. 또한, 다양한 상대 평가 방법(예: 정답 여부, 관련성 점수, 사용자 피드백)을 조합하여 사용하는 것이 성능 향상에 도움이 될 수 있습니다.\n\n## [**비판적 평가**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EB%B9%84%ED%8C%90%EC%A0%81-%ED%8F%89%EA%B0%80)\n\n### [**강점**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EA%B0%95%EC%A0%90)\n\n1. **데이터 효율성**: 소수의 훈련 샘플만으로도 성능을 향상시킬 수 있는 데이터 효율적인 방법을 제시합니다.\n2. **범용성**: 다양한 도메인에서 효과적인 성능 향상을 보입니다.\n3. **과적합 방지**: 경험적 지식을 활용하여 과적합을 방지할 수 있습니다.\n4. **리소스 효율성**: 파라미터 업데이트가 필요 없으므로 연산 비용과 메모리 사용량을 줄일 수 있습니다.\n\n### [**한계점과 개선 방향**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%9C%EA%B3%84%EC%A0%90%EA%B3%BC-%EA%B0%9C%EC%84%A0-%EB%B0%A9%ED%96%A5)\n\n1. **상대 평가 방법의 정교함**: 상대 평가 방법이 성능에 큰 영향을 미치므로, 이를 더 정교하게 설계할 필요가 있습니다. 특히, 자동화된 평가 방법을 개발하여 평가 과정의 효율성을 높이는 것이 중요합니다.\n2. **토큰 사전의 크기**: 토큰 사전의 크기가 성능에 영향을 미치므로, 최적의 크기를 찾는 것이 중요합니다. 동적으로 토큰 사전의 크기를 조절하는 방법을 연구할 필요가 있습니다.\n3. **토큰 사전의 내용**: 토큰 사전의 내용이 편향되거나 노이즈를 포함할 경우 성능 저하를 일으킬 수 있습니다. 토큰 사전의 품질을 개선하기 위한 연구가 필요합니다.\n4. **LLM 의존성**: Training-Free GRPO는 LLM의 성능에 크게 의존합니다. LLM의 성능이 낮을 경우 효과가 제한적일 수 있습니다.\n\n### [**재현성 평가**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%9E%AC%ED%98%84%EC%84%B1-%ED%8F%89%EA%B0%80)\n\n제안된 방법론은 공개된 데이터셋과 코드로 재현이 가능하며, 실험 설정이 명확하게 설명되어 있습니다. 그러나 상대 평가 방법의 구현이 간단하게 설명되어 있어, 이를 구체화하는 것이 필요합니다. 또한, 다양한 LLM 모델과 데이터셋에 대한 실험 결과를 추가하여 일반화 성능을 검증하는 것이 중요합니다.\n\n## [**향후 연구 방향**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%96%A5%ED%9B%84-%EC%97%B0%EA%B5%AC-%EB%B0%A9%ED%96%A5)\n\nTraining-Free GRPO는 다양한 도메인에서의 적용 가능성이 있습니다. 특히, 실시간으로 변화하는 환경에서의 문제 해결에 효과적일 수 있습니다. 향후 연구에서는 이러한 환경에서의 성능을 평가하고, 상대 평가 방법을 더 정교하게 설계하는 것이 필요합니다. 또한, 토큰 사전을 자동으로 생성하고 관리하는 방법을 연구하여 Training-Free GRPO의 효율성을 높이는 것이 중요합니다. 강화 학습과 결합하여 토큰 사전을 학습하는 방법도 연구할 가치가 있습니다.\n\n## [**실무 적용 가이드**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%8B%A4%EB%AC%B4-%EC%A0%81%EC%9A%A9-%EA%B0%80%EC%9D%B4%EB%93%9C)\n\nTraining-Free GRPO를 실무에 적용할 때는 다음과 같은 점을 고려해야 합니다:\n\n1. **토큰 사전의 크기**: 최적의 토큰 사전 크기를 찾는 것이 중요합니다. 이는 실험을 통해 조정할 수 있습니다. 다양한 크기의 토큰 사전을 사용하여 성능 변화를 관찰하고, 적절한 크기를 선택해야 합니다.\n2. **상대 평가 방법**: 상대 평가 방법을 정교하게 설계하여 성능을 최적화해야 합니다. 문제의 특성에 맞는 평가 지표를 선택하고, 필요에 따라 여러 평가 지표를 조합하여 사용해야 합니다.\n3. **데이터 효율성**: 소수의 훈련 샘플로도 성능을 향상시킬 수 있으므로, 데이터 수집 비용을 절감할 수 있습니다. 기존에 보유하고 있는 데이터를 최대한 활용하고, 필요한 경우 소량의 데이터를 추가적으로 수집하는 전략을 세워야 합니다.\n4. **모델 선택**: Training-Free GRPO는 LLM의 성능에 의존적이므로, 적절한 LLM 모델을 선택하는 것이 중요합니다. 문제의 복잡도와 필요한 지식 수준을 고려하여 LLM 모델을 선택해야 합니다.\n\n## [**결론**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EA%B2%B0%EB%A1%A0)\n\nTraining-Free GRPO는 매개변수 업데이트 없이도 LLM의 성능을 향상시킬 수 있는 새로운 방법론입니다. 이 방법론은 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정하며, 데이터 부족 문제를 해결하고, 과적합을 방지할 수 있습니다. 다양한 도메인에서의 성능 향상을 통해 LLM 에이전트의 활용 가능성을 넓히고, 데이터 효율적인 방식으로 성능을 향상시킬 수 있는 잠재력을 제시합니다. 특히, few-shot learning 환경에서 강력한 성능을 보이며, 리소스 제약이 있는 환경에서 유용하게 활용될 수 있습니다.\n\n## [**참고 자료**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C)\n\n- [논문 링크](https://arxiv.org/abs/2510.08191)\n- [코드 저장소](https://github.com/example-repo/Training-Free-GRPO)\n- 관련 자료: [DeepSeek-V3.1-Terminus 모델](https://example.com/deepseek-v3.1-terminus)\n- [LoRA(Low-Rank Adaptation) 논문](https://arxiv.org/abs/2106.09698)\n- [DPO(Direct Preference Optimization) 논문](https://arxiv.org/abs/2305.18290)\n- [AutoPrompt 논문](https://arxiv.org/abs/2003.10581)",
      "id": "8679d703-1cf9-4032-bb96-a091d081cc60",
      "createdAt": "2026-02-17T12:57:00.096Z",
      "updatedAt": "2026-02-17T12:58:07.254Z"
    },
    {
      "citeKey": "github2026",
      "url": "https://github.com/p-e-w/heretic",
      "type": "blog",
      "title": "GitHub - p-e-w/heretic: Fully automatic censorship removal for language models",
      "authors": [],
      "date": "",
      "description": "Fully automatic censorship removal for language models - p-e-w/heretic",
      "thumbnail": "https://opengraph.githubassets.com/dd65be10f860f311fd4d6f55e0ac2b9823c6002c86aebaf197f8cdf09e34ad91/p-e-w/heretic",
      "siteName": "GitHub",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "d9ab135b-062d-436c-8f1e-053c5d2ec76c",
      "createdAt": "2026-02-17T13:07:48.034Z",
      "updatedAt": "2026-02-17T13:07:48.034Z"
    },
    {
      "citeKey": "github2026a",
      "url": "https://github.com/ComposioHQ/composio",
      "type": "blog",
      "title": "GitHub - ComposioHQ/composio: Composio equips your AI agents & LLMs with 100+ high-quality integrations via function calling",
      "authors": [],
      "date": "",
      "description": "Composio equips your AI agents & LLMs with 100+ high-quality integrations via function calling - ComposioHQ/composio",
      "thumbnail": "https://opengraph.githubassets.com/800e1758f9f0cf3e976fd1fc1b8279d289a5b5fd0671659d3800bbfb75cdc4a4/ComposioHQ/composio",
      "siteName": "GitHub",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "18ac64c6-c275-4feb-8579-12134350ea2a",
      "createdAt": "2026-02-17T13:07:58.458Z",
      "updatedAt": "2026-02-17T13:07:58.458Z"
    },
    {
      "citeKey": "compound2026",
      "url": "https://limc.dev/log/article/dev/ai/ar-da-02-en-compound-engineering-introducing-it-using-codex-skills/",
      "type": "blog",
      "title": "\"Compound Engineering\" - Introducing it using Codex Skills - SnaqSh0t",
      "authors": [],
      "date": "",
      "description": "기록 남기는 곳",
      "thumbnail": "https://limc.dev/assets/og.png",
      "siteName": "SnaqSh0t",
      "tags": [],
      "collections": [],
      "notes": "# **\"Compound Engineering\" - Introducing it using Codex Skills**\n\n**AR-DA-02-EN**calendar_today2026-02-17 00:39#AGENTS #CODEX #AI\n\n## **Overview**\n\n- This note keeps the original `ce-*` files from `~/.agents/skills` as shared references and adds what each file solves.\n- The reason this post is long is that I kept the original skill contents instead of summarizing them away.\n- Start from sections 1 to 4, then refer to section 5 onward when needed for exact file details.\n\n## **1. Rules in your head evaporate**\n\nAt first, everyone says the same things.\n\n- \"We should always create a plan before starting\"\n- \"Reviews should always happen\"\n- \"The same bug should never come back\"\n\nThen timelines get tight, requests pile up, and parallel work starts piling up, and those rules blur quickly.\n\nAnd only when the cost returns, it starts to come back to you.\n\n- Scope changes, but implementation keeps going as before.\n- We ran tests, but never documented what was actually verified.\n- We reviewed, but never agreed on what was the highest-risk issue.\n- The bug is fixed, then appears again in almost the same shape the next week.\n\nThat is why I decided not to keep these as abstract sentences.\n\nI enforced them by turning AGENTS-like rules into Skills files.\n\n## **2. Turn rules into skill files**\n\nTo use this consistently across every project in my environment, I created these skills under `~/.agents/skills`.\n\n*I’ll describe this without leaking absolute paths, using home-directory notation.*\n\nThe idea is simple.\n\n- The loop is fixed to `Plan → Work → Review → Compound`.\n- It is based on the notes from [AR-DA-01 Compound Engineering Notes](https://limc.dev/log/article/dev/ai/ar-da-01-compound-engineering-%EB%A5%BC-%EC%9D%BD%EA%B3%A0/).\n- Each stage has explicit deliverables in files.\n- Stage transitions are blocked by explicit approval gates.\n  - I previously tried to enforce this only with [AGENTS.md](http://agents.md/), but it kept being bypassed and phases were mixed. ...\n\n## **3. Current skill composition**\n\nBelow is the skill-only directory structure used for the Compound-Engineering workflow.\n\ntextcontent_copy\n\n```\n~/.agents/skills/├── README.md├── ce-router/├── ce-plan/├── ce-review/└── ce-compound/\n```\n\nThe effect is immediate.\n\n- On a brand-new project, the same prompt is always used.\n- It is explicit when we should stop and wait for approval.\n\n## **4. [README.md](http://readme.md/): a map for this workflow**\n\n`README.md` gives context and minimal orientation for the toolkit.\n\nmdcontent_copy\n\n```\n## Skill set for this kit- ce-router (gate/routing): implicit allowed (auto-intervention where possible)- ce-plan (PLAN deliverables)- ce-work (WORK execution + mandatory Work Log)- ce-review (review + P1/P2/P3)- ce-compound (at least one carry-over artifact required)\n```\n\nThe effect is simple.\n\n- It saves time searching for \"what skill exists\".\n- It keeps the loop components visible in one place.\n- It naturally nudges people into moving stages in order.\n- The files are mainly for humans, not for agents.\n\n## **5. ce-router: entrypoint (routing + approval gate)**\n\nIn rapid, parallel-driven work, uncertainty about current stage is common.\n\nWhen \"what step are we in\" becomes unclear, the next stage usually unravels.\n\n`ce-router` exists to remove that ambiguity.\n\nmdcontent_copy\n\n```\n---name: ce-routerdescription: |  If the user request is unclear about which stage it belongs to among PLAN/WORK/REVIEW/COMPOUND,  or asks for implementation/fixes/tests/deployments without approved PLAN, this skill enforces  the Compound Engineering loop by forcing stage routing and explicit handoff.  For requests like \"implement\", \"fix\", \"refactor\", \"run tests\", \"deploy\",  it explicitly tells the assistant the current phase, approval state, and next skill to call.---## Core Rules- All responses are written in Korean.- Code comments are always written in Korean.- Single-person development is assumed; final decisions belong to the user.- AI leads planning → implementation → testing → documentation, and stage transitions happen only after explicit user approval.  - Exception: when the user explicitly requests uninterrupted execution.## Routing Objective- Classify the request into exactly one stage.  - PLAN / WORK / REVIEW / COMPOUND- Ask only 1~3 high-impact questions if more information is needed.## Approval Gate (Mandatory)- Do not move to WORK without an approved PLAN.- Even with an approved PLAN, if stage transition is not approved:  - Stop and ask one sentence: \"Can I continue to the next stage?\"- If the user requests uninterrupted execution:  - Continue, except when Plan Drift (scope/policy/invariant changes) occurs; then stop and re-confirm with 1~3 questions.## Output Format (Mandatory)1) Current-state summary (3~6 lines)   - Interpreted objective   - Current stage (estimated or confirmed)   - Approval/delegation status2) One next stage and reason3) Questions 1~3 (only if needed)4) Input block template for the next stage   - Copy-paste ready5) Final line: `CE ROUTING COMPLETE`\n```\n\n### **5.1. Effect from this skill**\n\n- It blocks accidental transitions from PLAN to WORK.\n- It turns phase change from \"implicit mood\" into \"explicit approval\".\n- It caps follow-up questions to 1\\~3 only.\n\n## **6. ce-plan: fixing plan as an explicit artifact**\n\nThe purpose of Plan is not a giant design doc.\n\nIt is the minimum guardrail that prevents Work from drifting.\n\nmdcontent_copy\n\n```\n---name: ce-plandescription: |  Compose the PLAN stage outputs for Compound Engineering.  Without implementing or running tests, write goals, scope, DoD, defect mapping,  invariants, policy, checklist, test plan, and risk controls in a high-quality plan.  Ask up to 3 questions only when uncertain.---## Mode Lock- PLAN-only.- No implementation, code edits, or test execution.- Do not make unsupported assumptions; if evidence is missing, label it as assumption and escalate for approval.## User Input (required)Users provide as much as available from the format below. If missing, ask 1~3 questions.- Project:- Stack/runtime:- Work ID:- Work title:- Current stage: PLAN- SoT/reference files (optional): (path list)- Target requirements/defects: (list)- Invariants/policies: (list)- Scope include / exclude- Non-negotiable invariants: (list)- (Optional) Code hints: file paths/modules/endpoints## Quality Standards (Checklist)- First Principles: refine requirements into atomic units- Invariants First: lock invariant protection as strategy core- Failure-Oriented: design with failure/exception/boundary first- Minimal Complexity: start from the simplest approach- Tight Loop: enforce stage-by-stage verification loops- Observability: include logging/metrics/error-message improvement points## Evidence Rule (Mandatory)- If possible, attach evidence as \"file path + line range\" for each claim.- If evidence cannot be verified yet, write:  - (1) Unverified evidence  - (2) Candidate files to verify  - (3) Commands/methods to verify  in the PLAN document.## Output Format (Mandatory)Must include the following 13 sections in order.1) Goal / scope / out-of-scope2) Defect or requirement matrix3) Invariant protection strategy4) Policy decision table5) Completion criteria (DoD, measurable)6) Candidate changed files + rationale7) Stagewise Work checklist (W1~Wn, validation command per stage)8) Test plan (Red → Green split, happy/failure/boundary)9) Risks/regressions (P1/P2/P3) + response   - P1: data/security/crash/critical regression   - P2: durability / major quality / high usability impact   - P3: documentation / cleanup / improvements10) Plan Drift rules (stop/approval conditions)11) Review priority criteria12) Compound candidates (minimum 1)13) User approval points (1~3)### Top summary (required)- Add a top-of-document summary within 5 lines.### Bottom sentinel (required)- Final line: `PLAN COMPLETE — REQUEST WORK APPROVAL`\n```\n\n### **6.1. Effects of this skill**\n\n- Goals/scope/constraints are fixed as written agreement.\n- DoD makes \"done\" measurable and reviewable.\n- It forces failure/exception/boundary thinking first, reducing late-stage debugging.\n\n## **7. ce-work: combining execution, verification, and logs**\n\nIn Work, two mistakes happen again and again.\n\n- Continuing beyond plan without noticing scope drift.\n- Running checks but leaving no evidence of what was verified.\n\n`ce-work` focuses on both at once.\n\nmdcontent_copy\n\n```\n---name: ce-workdescription: |  Execute the WORK stage based on an approved PLAN.  Force a cycle of one checklist item at a time, verify what was done,  and enforce keeping a Work Log.  Stop and request approval immediately when Plan Drift is detected.---## Preconditions (Gate)- Do not execute WORK without approved PLAN.  - Instead, ask 1~3 missing questions or guide to $ce-plan.- Continuous execution is only allowed when user explicitly says no intermediate confirmation.  - Stop immediately and re-confirm approval if Plan Drift occurs.## Basic Rules- All responses are in Korean.- Code comments are always in Korean.- No work outside the approved PLAN scope.- Enforce meaningful unit commits (commit message must include what and why).## User Input- Work ID:- Work title:- Current stage: WORK- Approved PLAN: (paste document path or summary)- (Optional) Branch/PR strategy:- (AGENTS.md at repo root) run/test commands:## Execution Unit (Mandatory)1) Select one checklist item2) Implement within PLAN scope3) Verify (tests/lint/type-check/manual scenarios where possible)4) Commit with what/why5) Update Work Log## Plan Drift Handling (Mandatory)- Pause immediately if any of these occur:  - Scope inversion (include/exclude flipped)  - Possible fixed-policy violation  - Risk of breaking invariants  - Need to redefine DoD- On pause, provide:  - Before/after state  - Why the change happened  - Impact (P1/P2/P3)  - 1~2 alternatives  - One-line re-approval request## Work Log (Mandatory)- At WORK completion, leave at least one:  - docs/plans/<work-id>-worklog.md creation  - or add a `## Work Log` section at the end of docs/plans/<work-id>-plan.md### Work Log Minimum Items- Progress summary (3~7 lines)- Final changed file list- Checklist completion status ([x])- Commands run and results (only what ran)- Commit hash or PR number/link- Final 3 lines at work end:  - Completed checklist items in this WORK:  - Validations passed in this state:  - Risk points to watch in Review (if any):## Recommended Output Format (Operational)- Checklist items completed in this turn- Changed files + core reason- Validation commands/results- Work Log update summary- Next checklist proposal- Final line: `WORK STATUS SHARED — REQUEST NEXT STAGE APPROVAL`\n```\n\n### **7.1. Effects of this skill**\n\n- Work is broken into small completion slices instead of a single large pass.\n- Plan Drift is blocked early, reducing late-stage scope inflation.\n- Work Log creation is enforced and reduces handoff cost.\n\n## **8. ce-review: turning work into alignment**\n\nActual review is where we decide whether a loop outcome is ready.\n\nThe key is forcing prioritization.\n\n- P1: must-fix now.\n- P2: should be fixed if possible in this cycle.\n- P3: can be deferred.\n\nmdcontent_copy\n\n```\n---name: ce-reviewdescription: |  Run the REVIEW stage after WORK.  Start with a self PR review, and prioritize checks in order: bugs/edges, risks,  regressions, then readability/maintainability.  Classify issues as P1/P2/P3 and summarize so the user can choose what to fix next.---## User Input- Work ID:- Work title:- Current stage: REVIEW- Approved PLAN (path or summary):- Change summary (if available):- Commit/PR identifier (if available):- Validation results (if available):## Review Priority (Mandatory)1) Bug likelihood / edge cases2) Risks (security, data loss, performance, operations)3) Regression risk (impact on existing behavior)4) Readability / maintainability## Issue Classification (Mandatory)- P1: Must fix now (data/security/crash/critical regression)- P2: Fix if possible this cycle (usability/durability/important quality)- P3: Documentation/cleanup/improvements (deferable)## Evidence Rule (Mandatory)- Use file path + line range as evidence whenever possible.- If verification is not complete, mark as \"evidence not verified\" and list candidate files/lines to check.## Output Format (Mandatory)1) Change summary (3~7 lines)2) DoD completion check (by each PLAN DoD item)3) P1 / P2 / P3 issue list   - For each: symptom / impact / reproduction or condition / evidence (file:line) / recommended fix4) Additional test/observability points (happy/failure/boundary)5) User decisions required (1~3)6) Final line: `REVIEW COMPLETE — REQUEST APPROVAL FOR NEXT STAGE (COMPOUND OR ADDITIONAL WORK)`\n```\n\n### **8.1. Effects of this skill**\n\n- Review quality stays stable by locking priorities to bug/risk/regression/readability.\n- P1/P2/P3 makes decision-making explicit.\n- Evidence includes file/line references, reducing rework.\n\n## **9. ce-compound: one more pass for reusability**\n\nWithout this final stage, loops often stop at \"we did the work\".\n\n`ce-compound` pushes it one step further.\n\nmdcontent_copy\n\n```\n---name: ce-compounddescription: |  Run the COMPOUND stage.  Every work cycle must leave at least one carry-over asset for the next cycle,  such as tests, docs, guardrails, rules, or observability improvements.  Answer the question: if this issue happens again, will tests/rules catch it automatically?  If not, leave at least one artifact that makes future discovery easier.---## Input- Work ID:- Work title:- Current stage: COMPOUND- Final change summary:- Proposed carry-over artifact candidates (if any):## Mandatory Rules- Propose at least one carry-over artifact (preferably immediately actionable).- Answer these questions:  - If this issue happens again, will tests/rules/guards catch it automatically?  - If not, what should be left so it is easier to find next time?## Recommended Artifact Paths- docs/solutions/<slug>.md (recommended)- Or:  - Add regression tests  - Strengthen AGENTS.md rules (short, strict)  - Update review checklist  - Improve observability (logs/error messages/tracing points)## Output Format (Mandatory)1) 1~3 candidate artifacts (priority order)2) At least one should be immediately actionable   - propose file path + doc/test skeleton3) Expected effect (why next work is easier)4) User approval points (1~3)5) Final line: `COMPOUND COMPLETE — REQUEST REPEAT APPROVAL`\n```\n\n### **9.1. Effects of this skill**\n\n- It forces explicit check: will the issue be automatically caught next time?\n- It requires at least one reusable carryover item (test/rules/docs/observability).\n- Even one persisted artifact lowers the cost of repeated mistakes.\n\n## **10. Conclusion**\n\nThe reason I keep this structure is simple.\n\n- The loop protects people by encoding the process in files, not vibes.\n- It preserves evidence, not just execution.\n- Every cycle leaves the workspace easier to continue from.\n\nPlan → Work → Review → Compound is not just a set of names.\n\nIt is a method for locking approval, verification, and anti-regression into files.",
      "id": "2e5d0704-e8b1-435c-aa9e-a021621f38f5",
      "createdAt": "2026-02-17T13:08:33.714Z",
      "updatedAt": "2026-02-17T13:08:50.410Z"
    },
    {
      "citeKey": "github2026b",
      "url": "https://github.com/maxritter/claude-pilot",
      "type": "blog",
      "title": "GitHub - maxritter/claude-pilot: Claude Code is powerful. Pilot makes it reliable. Start a task, grab a coffee, come back to production-grade code. Tests enforced. Context preserved. Quality automated.",
      "authors": [],
      "date": "",
      "description": "Claude Code is powerful. Pilot makes it reliable. Start a task, grab a coffee, come back to production-grade code. Tests enforced. Context preserved. Quality automated. - maxritter/claude-pilot",
      "thumbnail": "https://repository-images.githubusercontent.com/1078701786/7d7aea0c-a0be-4d67-a26a-2d31bb680c4a",
      "siteName": "GitHub",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "e70c1a1a-9a8d-4e8b-8c12-5e3f3424eafb",
      "createdAt": "2026-02-17T13:09:54.527Z",
      "updatedAt": "2026-02-17T13:09:54.527Z"
    },
    {
      "citeKey": "github2026c",
      "url": "https://github.com/brendanhogan/hermitclaw/",
      "type": "blog",
      "title": "GitHub - brendanhogan/hermitclaw",
      "authors": [],
      "date": "",
      "description": "Contribute to brendanhogan/hermitclaw development by creating an account on GitHub.",
      "thumbnail": "https://opengraph.githubassets.com/3873e60cbce772c3f4879a6d33ed848f334adf28c3d2975389edda0a4be22eca/brendanhogan/hermitclaw",
      "siteName": "GitHub",
      "tags": [],
      "collections": [],
      "notes": "HermitClaw는 데스크탑의 특정 폴더만 접근할 수 있도록 제한된 AI 에이전트입니다.\\\n해당 폴더 안에서만 작동하며, 파일을 넣어주면 코드를 짜거나 웹 검색, 글쓰기, 리서치 등을 스스로 수행합니다.\\\n몇 초마다 자체적인 '기분'이나 '계획'에 따라 짧게 사고하고 행동하며, 기억은 Smallville 논문(Generative Agents, Park et al. 2023)을 참고해 시간, 중요도와 함께 저장됩니다.\\\n중요한 생각이 일정량 쌓이면 잠시 멈춰 자신이 한 생각을 되돌아보고 요약까지 수행합니다.",
      "id": "f2087e4a-9dc5-4d6e-bef7-0a47c835e104",
      "createdAt": "2026-02-17T13:10:43.201Z",
      "updatedAt": "2026-02-17T13:10:54.321Z"
    },
    {
      "citeKey": "yuchen2026",
      "url": "https://arxiv.org/abs/2602.12394",
      "type": "academic",
      "title": "Synthetic Interaction Data for Scalable Personalization in Large Language Models",
      "authors": [
        "Ma",
        "Yuchen",
        "Huang",
        "Yue",
        "Wang",
        "Wenjie",
        "Luo",
        "Xiaonan",
        "Zhang",
        "Xiangliang",
        "Feuerriegel",
        "Stefan"
      ],
      "date": "2026/02/12",
      "description": "Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "PersonaGym",
      "id": "4bd931e6-e1f3-4cd0-a6b1-326dbcec1b8b",
      "createdAt": "2026-02-17T13:11:50.665Z",
      "updatedAt": "2026-02-17T13:11:58.565Z"
    },
    {
      "citeKey": "ruihan2026",
      "url": "https://arxiv.org/abs/2602.12662",
      "type": "academic",
      "title": "Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents",
      "authors": [
        "Yang",
        "Ruihan",
        "Ye",
        "Fanghua",
        "We",
        "Xiang",
        "Zhao",
        "Ruoqing",
        "Luo",
        "Kang",
        "Xu",
        "Xinbo",
        "Zhao",
        "Bo",
        "Ma",
        "Ruotian",
        "Wang",
        "Shanyi",
        "Tu",
        "Zhaopeng",
        "Li",
        "Xiaolong",
        "Yang",
        "Deqing",
        "Linus"
      ],
      "date": "2026/02/13",
      "description": "Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [
        "Agent",
        "Level thinking"
      ],
      "collections": [
        "Agent"
      ],
      "notes": "이 논문은 **대형 언어 모델(LLM) 에이전트의 사고 방식 적응**을 다루고 있습니다. 핵심 요점은 다음과 같습니다:\n\n### 문제 인식\n\n- 기존 LLM 에이전트는 **고정된 사고 패턴**을 사용합니다:\n\n  - \"즉각 반응형(non-thinking)\" 모델은 모든 단계에서 단순 반응만.\n\n  - \"깊은 사고(thinking)\" 모델은 모든 단계에서 복잡한 추론만.\n\n- 이런 방식은 **비효율적**입니다. 단순한 단계에도 불필요하게 많은 토큰을 쓰거나, 복잡한 단계에서 단순 반응으로 실패합니다.\n\n### 제안된 해결책: **COGROUTER**\n\n- **ACT-R 인지 이론**을 기반으로, 단계별로 사고 깊이를 조절하는 프레임워크.\n\n- 네 가지 인지 수준(L1–L4)을 정의:\n\n  - L1: 직관적 반응\n\n  - L2: 상황 인식\n\n  - L3: 경험 반영\n\n  - L4: 전략적 계획\n\n- **2단계 학습 과정**:\n\n  1. **COSFT (Cognition-aware Supervised Fine-tuning)**: 각 수준별 사고 패턴을 안정적으로 학습.\n\n  2. **COPO (Cognition-aware Policy Optimization)**: 강화학습을 통해 단계별로 적절한 사고 깊이를 선택. 핵심은 **행동 예측의 확신(confidence**)을 기준으로 보상 재분배.\n\n### 주요 성과\n\n- **ALFWorld**와 **ScienceWorld** 벤치마크에서 평가.\n\n- **Qwen2.5-7B** 모델 기반으로:\n\n  - 평균 성공률 **82.3%** 달성.\n\n  - GPT-4o 대비 +40.3%, OpenAI-o3 대비 +18.3%, GRPO 대비 +14.0% 향상.\n\n  - 토큰 사용량은 기존 대비 **62% 절감**.\n\n- COPO는 기존 RL 방법들이 빠지기 쉬운 **“깊은 사고(L4)로의 붕괴**”를 방지하고, 단계별로 적절한 사고 깊이를 배분.\n\n### 기여 요약\n\n1. LLM 에이전트의 **인지 경직성(cognitive rigidity)** 문제를 정의.\n\n2. **COGROUTER** 프레임워크 제안: 다단계 인지 수준 + 2단계 학습.\n\n3. **COPO 알고리즘** 도입: 확신 기반 보상 재분배로 단계별 사고 깊이 최적화.\n\n즉, 이 논문은 **“모든 단계에서 똑같이 깊게 생각하는 대신, 상황에 맞게 사고 깊이를 조절하는 LLM 에이전트**”를 만드는 방법을 제시하고, 실제로 더 높은 성능과 효율성을 입증했습니다.",
      "id": "005fafba-f4c0-4c87-a920-5d0701ea9737",
      "createdAt": "2026-02-17T13:13:30.854Z",
      "updatedAt": "2026-02-18T11:25:00.796Z"
    },
    {
      "citeKey": "junjie2026",
      "url": "https://arxiv.org/abs/2602.12852",
      "type": "academic",
      "title": "WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning",
      "authors": [
        "Wang",
        "Junjie",
        "Xie",
        "Zequn",
        "Yang",
        "Dan",
        "Feng",
        "Jie",
        "Shen",
        "Yue",
        "Sun",
        "Duolin",
        "Long",
        "Meixiu",
        "Jiao",
        "Yihan",
        "Tan",
        "Zhehao",
        "Wang",
        "Jian",
        "Wei",
        "Peng",
        "Gu",
        "Jinjie"
      ],
      "date": "2026/02/13",
      "description": "Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [
        "Agent",
        "Web",
        "WebAgent"
      ],
      "collections": [
        "Agent"
      ],
      "notes": "",
      "id": "3bf958c8-323e-42d3-b492-4988ef490dda",
      "createdAt": "2026-02-17T13:16:15.078Z",
      "updatedAt": "2026-02-18T11:24:43.352Z"
    },
    {
      "citeKey": "futing2026",
      "url": "https://arxiv.org/abs/2602.11748",
      "type": "academic",
      "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
      "authors": [
        "Wang",
        "Futing",
        "Yan",
        "Jianhao",
        "Luo",
        "Yun",
        "Cui",
        "Ganqu",
        "Wang",
        "Zhi",
        "Qu",
        "Xiaoye",
        "Zhang",
        "Yue",
        "Cheng",
        "Yu",
        "Lin",
        "Tao"
      ],
      "date": "2026/02/12",
      "description": "Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.\n  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.\n  To bridge this gap, we propose Length-Incentivized Exploration(\\method).\n  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.\n  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration.\n  As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [
        "Agent"
      ],
      "notes": "1\\. test-time scaling에서 중요한 능력을 in-context exploration으로 정의함. 한 번의 연속 컨텍스트 안에서 가설을 여러 개 생성하고, 검증하고, 수정하면서 상태를 넓게 탐색하는 능력임. \n\n2\\. 이 능력을 state coverage 관점으로 분석하고, 긴 reasoning trajectory가 더 넓은 state coverage를 만들지만 autoregressive 샘플링에서는 긴 시퀀스가 나올 확률이 지수적으로 줄어 “shallow exploration trap”에 빠진다고 정식화함. \n\n3\\. 표준 RLVR/GRPO류 학습은 어느 정도 길이를 늘리는 경향이 있어도, 길이를 “정보 탐색”이 아니라 “반복/군더더기”로 채워서 state density가 떨어지는 문제가 생긴다고 보고, 길이만 늘리면 해결되지 않는다고 주장함. \n\n4\\. 해결책으로 LIE(length-incentivized exploration)라는 reward shaping 레시피를 제안함. 핵심은 2단 구성임: (a) 길이 기반 보상으로 탐색 용량(더 긴 trajectory)을 강제로 확보하고, (b) redundancy penalty로 반복을 억제해 같은 길이에서 더 많은 “서로 다른 상태”를 밟게 만들어 실제 state coverage를 키움. \n\n5\\. 길이 보상은 샘플별로 현재 정책이 원래 내는 길이를 기준으로 target length를 두고, 최소한 그보다 더 길게(ΔL만큼) 생성하도록 유도하는 형태로 설명됨. 이때 길이만 인센티브하면 distinct ratio가 떨어지고 반복이 늘어나는 부작용이 실험에서 확인됨. \n\n6\\. redundancy penalty는 “긴데 실질 상태를 안 늘리는 출력”을 깎아, 길이 보상이 유도한 추가 토큰이 반복 채우기가 아니라 새로운 가설/검증/수정 같은 탐색 행동으로 쓰이게 만드는 역할로 제시됨. \n\n7\\. 실험은 Qwen3, Llama 계열에서 수행되며, LIE가 in-domain reasoning 태스크 평균 +4.4%, out-of-domain 벤치 평균 +2.7% 향상을 보고함.",
      "id": "e05bfd84-50c9-41af-b0e1-b35fb4505588",
      "createdAt": "2026-02-17T13:23:22.598Z",
      "updatedAt": "2026-02-18T06:09:57.490Z"
    },
    {
      "citeKey": "github2026d",
      "url": "https://github.com/shanraisshan/claude-code-best-practice",
      "type": "blog",
      "title": "GitHub - shanraisshan/claude-code-best-practice: practice made claude perfect",
      "authors": [],
      "date": "",
      "description": "practice made claude perfect. Contribute to shanraisshan/claude-code-best-practice development by creating an account on GitHub.",
      "thumbnail": "https://opengraph.githubassets.com/37f8fdd5b0c9f6455962ba78f2fd14ab1612ae29cdb55cde00c9a5653e5c5d5b/shanraisshan/claude-code-best-practice",
      "siteName": "GitHub",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "2634765f-01d3-4d85-b8a2-a3f75f474d26",
      "createdAt": "2026-02-18T13:38:58.145Z",
      "updatedAt": "2026-02-18T13:38:58.145Z"
    },
    {
      "citeKey": "when2026",
      "url": "https://transformer-circuits.pub/2025/linebreaks/index.html",
      "type": "blog",
      "title": "When Models Manipulate Manifolds: The Geometry of a Counting Task",
      "authors": [],
      "date": "",
      "description": "We find geometric structure underlying the mechanisms of a fundamental language model behavior.\n\n숫자만 보는 AI는 어떻게 '공간'을 이해하는가: Claude의 기하학적 사고방식\n1. 서론: 텍스트의 세계에서 '시각'이 없는 AI의 도전\n인간이 종이에 글을 쓸 때, 우리는 남은 여백을 직관적으로 감지하며 줄을 바꿀 시점을 결정합니다. 하지만 인공지능(AI)에게는 물리적인 '눈'이 없습니다. 언어 모델이 입력받는 유일한 정보는 텍스트를 숫자로 치환한 '정수(토큰)의 시퀀스'뿐입니다. 모델에게 문장은 시각적인 형태가 아니라 그저 끝없이 이어지는 추상적인 숫자의 나열입니다.\n그렇다면 Anthropic의 Claude 3.5 Haiku와 같은 모델은 어떻게 특정 너비 제한이 있는 문서에서 정확한 지점에 줄바꿈(Linebreaking)을 예측하는 것일까요? 이는 단순히 다음 단어를 통계적으로 맞히는 문제를 넘어, 모델이 텍스트 기반 환경에서 스스로 '지각 능력(Perceptual abilities)'을 구축해야 함을 의미합니다. 최근 연구에 따르면, Claude는 이 문제를 해결하기 위해 내부적으로 고도로 정교한 기하학적 구조를 설계하여 자신만의 '시각'을 만들어내고 있습니다.\n--------------------------------------------------------------------------------\n2. 통찰 1: 데이터는 단순한 숫자가 아니라 '물결치는 곡선(Manifold)'이다\nAI 내부에서 '글자 수를 세는 작업'은 우리가 흔히 생각하는 1, 2, 3 식의 단순한 1차원 카운터가 아닙니다. 연구진은 Claude의 잔차 스트림(Residual Stream)을 분석한 결과, 글자 수 정보가 6차원 정도의 하위 공간 속에서 소용돌이치는 1차원 매니폴드(Manifold) 형태로 존재한다는 사실을 발견했습니다.\n왜 모델은 단순히 직선 형태의 숫자를 저장하지 않고 복잡한 곡선을 선택했을까요? 여기에는 '용량(Capacity)'과 '해상도(Resolution)' 사이의 정교한 트레이드오프가 숨어 있습니다. 만약 모델이 1차원 직선(Ray) 상에 150개의 숫자를 배치한다면, 숫자 사이의 구분을 위해 벡터의 크기가 기하급수적으로 커지는 '노름 폭발(Norm explosion)'이 발생하거나, 반대로 숫자들이 너무 밀집되어 149와 150을 구분하지 못하게 됩니다. Claude는 고차원 공간 속에서 선을 구부리고 물결치게 만듦으로써(Rippled), 벡터의 크기를 일정하게 유지하면서도 각 지점 간의 거리를 확보하는 영리한 전략을 택했습니다.\n\"이러한 물결 모양의 매니폴드는 용량 제약(차원수)과 서로 다른 스칼라 값의 구별 가능성(곡률) 사이에서 최적의 균형을 유지합니다.\"\n이 과정에서 흥미로운 현상인 **'링잉(Ringing)'**이 관찰됩니다. 이는 고곡률의 매니폴드를 저차원에 투영할 때 발생하는 간섭 현상으로, 마치 신호 처리에서의 깁스 현상(Gibbs phenomenon)처럼 나타납니다. 또한, 숫자가 커질수록 모델의 '시야'가 조금씩 흐려지는 '확장(Dilation)' 현상도 발견되었는데, 이는 큰 숫자를 인식할 때 해상도가 낮아지는 인간의 수치 지각 방식과 놀라울 정도로 흡사합니다. 이 매니폴드를 3차원 PCA로 시각화하면 마치 **'야구공의 실밥(Baseball seam)'**과 같은 독특한 위상적 구조를 띱니다.\n--------------------------------------------------------------------------------\n3. 통찰 2: '비틀기(Twist)' 연산을 통한 경계 감지\n모델이 현재 위치를 파악했다면, 다음 단계는 '언제 줄이 끝나는가'를 감지하는 것입니다. 이를 위해 Claude는 **'QK(Query-Key) 트위스트'**라고 불리는 기하학적 메커니즘을 사용합니다.\n이 메커니즘에서 **쿼리(Query)**는 '현재의 글자 수' 매니폴드를, **키(Key)**는 '전체 줄 너비' 매니폴드를 담당합니다. 특정 어텐션 헤드는 이 두 매니폴드를 기하학적으로 회전(Twist)시켜, 현재 글자 수(i)가 줄 너비(k)에 도달하기 직전(k = i + \\epsilon)의 특정 오프셋에서 두 곡선이 수학적으로 완벽하게 정렬되도록 만듭니다. 이때 두 벡터의 내적이 최대화되면서 모델은 \"곧 줄바꿈이 필요하다\"는 강력한 신호를 감지합니다.\n이는 생물학적 뇌에서 포유류가 공간을 탐색할 때 벽이나 장애물 같은 특정 경계에 도달하면 활성화되는 **'경계 세포(Boundary cells)'**의 기능과 매우 흡사합니다. AI가 텍스트라는 추상적 공간에서 생물학적 지각 시스템과 동일한 해법을 스스로 진화시킨 셈입니다.\n--------------------------------------------------------------------------------\n4. 통찰 3: AI도 '착시'를 겪는다 (학습된 편향의 부작용)\n인간이 주변 맥락 때문에 사물의 크기를 오해하는 착시를 겪듯, Claude 역시 특정 문자열을 만날 때 공간 지각에 실패하는 **'시각적 착시(Visual Illusions)'**를 보입니다.\n가장 대표적인 사례는 문장 중간에 @@나 `와 같은 기호가 삽입될 때입니다. 이러한 기호가 나타나면 줄바꿈을 예측하는 어텐션 헤드의 시선이 흩어지며 줄바꿈 예측 확률이 급격히 떨어집니다. 연구 결과, 이는 모델의 단순한 버그가 아니라 '학습된 사전 지식(Learned Priors)'의 오적용임이 밝혀졌습니다.\n예를 들어, 모델은 학습 데이터인 'git diff' 코드에서 @@가 새로운 코드 블록의 시작을 알리는 구분자로 쓰인다는 것을 배웠습니다. 따라서 일반적인 문장 속에서도 @@를 만나면 \"여기서부터 줄 세기를 다시 시작해야 한다\"는 강한 편향이 작동하여 현재의 글자 수 계산 메커니즘을 방해(Distract)하는 것입니다. 이는 AI의 오류가 무작위적인 실수가 아니라, 자신이 배운 세상의 규칙을 고수하려다 발생하는 지적인 부작용임을 시사합니다.\n--------------------------------------------------------------------------------\n5. 통찰 4: '복잡성 세금'을 줄이는 분산형 기하학 알고리즘\nClaude의 지각 능력은 단일 뉴런이 아닌, 수많은 어텐션 헤드가 협력하여 완성하는 **'분산형 알고리즘'**을 통해 구현됩니다. 연구진은 이를 이해하기 위해 **'복잡성 세금(Complexity Tax)'**이라는 흥미로운 개념을 제시합니다. 수만 개의 개별 뉴런을 하나씩 분석하는 것은 너무나 고통스럽고 복잡하지만(세금이 높음), 이를 하나의 '기하학적 매니폴드'로 통합해 이해하면 모델의 작동 원리를 훨씬 단순하고 명확하게 파악할 수 있다는 논리입니다.\n이 분산 알고리즘은 레이어를 거치며 진화합니다.\n레이어 0 (감각의 시작): 초기 어텐션 헤드들은 각기 다른 위치 오프셋을 담당하며 기초 정보를 생성합니다. 이때 각 헤드의 출력은 아직 구부러지지 않은 직선(Ray)의 형태에 가깝습니다.\n레이어 1 (정교한 지각): 레이어 0에서 전달된 직선적 정보들을 결합하여 비로소 복잡한 곡률을 가진 '물결치는 매니폴드'를 구축합니다. 이 단계를 거치며 모델의 공간 해상도는 비약적으로 향상됩니다.\n이처럼 초기 레이어들은 단순한 토큰 해독기가 아니라, 데이터를 고차원적인 기하학적 세계로 변환하는 정교한 **'감각 입력 단계'**로서 기능합니다.\n--------------------------------------------------------------------------------\n6. 결론: '지각(Perception)'의 관점에서 본 언어 모델의 미래\n이번 연구는 언어 모델이 단순히 단어를 통계적으로 나열하는 기계가 아님을 증명합니다. Claude는 비록 눈은 없지만, 내부적으로 구축한 매니폴드와 기하학적 연산을 통해 누구보다 선명하게 텍스트의 '공간'을 보고 있습니다. 줄바꿈이라는 사소해 보이는 작업 이면에 이토록 거대하고 아름다운 기하학적 세계가 숨겨져 있다는 사실은 AI의 내부 메커니즘이 생물학적 지능의 심오함에 한 걸음 더 다가섰음을 보여줍니다.\n우리가 보는 평범한 텍스트의 이면에서, AI는 또 어떤 복잡한 기하학적 세계를 구축하고 있을까요? 모델의 초기 레이어를 '지각 과정'으로 재정의하는 이 새로운 관점은, 우리가 AI를 단순한 도구가 아닌 세상을 나름의 방식으로 '보는' 하나의 존재로 이해하게 만드는 중요한 이정표가 될 것입니다. AI가 구축한 이 보이지 않는 공간의 지도를 읽어내는 여정은 이제 막 시작되었을 뿐입니다.",
      "thumbnail": "https://transformer-circuits.pub/images/linebreaks-hero.png",
      "siteName": "Transformer Circuits",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "289b0b42-200f-459e-ad70-38c47d76caf4",
      "createdAt": "2026-02-18T13:39:58.956Z",
      "updatedAt": "2026-02-18T13:39:58.956Z"
    },
    {
      "citeKey": "topical2026",
      "url": "https://www.linkedin.com/posts/guijin-son-4909331bb_topical-funding-opportunity-awar-us-activity-7429009467175759872-xKnd/?utm_medium=ios_app&rcm=ACoAAApnkUIBLoV28tnJ8s4neHcMwZ9eNWN2SD0&utm_source=social_share_send&utm_campaign=share_via",
      "type": "linkedin",
      "title": "Topical Funding Opportunity Awar... | U.S. DOE Office of Science(SC) | GUIJIN SON",
      "authors": [],
      "date": "",
      "description": "최근 해외에서 조명되고 있는 AI 평가용 데이터셋 FirstProof가 등장하게 된 배경과 AI for Math/Science 트렌드, 이를 반영한 스타트업 투자와 Genesis Mission을 간단하게 정리해 봤습니다. \n\n--- \n\n수학은 언어 모델을 평가하기 위해 가장 많이 사용되는 방법 중 하나로 자리 잡았습니다. 가장 핵심적인 이유는 타 분야와 달리 단답형으로 잘 만들어진 일부 문제의 경우 “그럴듯한” 답변과 “정답인” 답변을 구별하는 게 매우 쉬워서이기도 합니다. 수학에서는 정답이 맞았는가를 바탕으로 번지르르한 풀이와 옳은 풀이를 비교적 저렴하고 뚜렷하게 구분해 낼 수 있습니다.\n\n최근에는 이 평가가 단순히 “문제를 푸냐/못 푸냐”를 넘어, 연구 수준의 수학에서 모델이 어떤 역할을 할 수 있느냐로 확장되는 흐름입니다. 대표적으로 FrontierMath는 풀기 어렵지만 정답이 명확하여 채점 가능한 고난도 문제들을 대량으로 구성해 성능을 가늠하려는 쪽에 가깝고, IMProofBench는 아예 증명(proof) 작성 능력과 연구 현장에서의 작업 흐름(문헌 탐색, 도구 활용 등)을 더 직접적으로 평가하려는 방향성을 갖고 있습니다. \n(https://lnkd.in/gETXzjk3) (https://lnkd.in/g5BZ-SGh)\n\n26년 1월에는 ChatGPT와 Aristotle 두 인공지능 모델이 Erdős Problem #205를 풀어내며 인공지능이 단순히 학습 데이터에서 \"본\" 문제를 그대로 푸는 수준에서, 조금이더라도 \"새로운 생각\"을 해야 하는 영역으로 나아가고 있음을 보였습니다. 이러한 AI for Math에 대한 관심이 AI 연구자들뿐만 아니라 수학자들에게도 퍼져 나가며 First Proof라는 프로젝트로 이어졌습니다. First Proof는 11명의 수학자들이 본인 연구 과정에서 실제로 등장한, 그리고 아직 인터넷에 공유 되지 않은, 10개의 문제를 공개하며, 현재 인공지능 모델들이 약 일주일의 제한 시간 동안 얼마나 제대로 된 증명을 생성해 낼 수 있는지 평가하는 프로젝트입니다.  (https://lnkd.in/gAdPNZKi) (https://1stproof.org/)\n\nOpenAI가 2월 13일에 위 10문제에 대한 자사 모델의 시도를 공개했으며 최소한 6개 문제에 대한 정답을 찾은 것 같다고 주장했고, 아직 공식적인 검토가 이루어지지는 않았지만, 인터넷에서는 6문제까지는 아니더라도 일부 문제 에 대해서는 문제를 푸는데 필요한 논문들을 제대로 인용하며 정답일 가능성이 높아 보인다는 의견이 있습니다.  (https://lnkd.in/gAhua7zf) (https://lnkd.in/gqcHewxM)\n\n최근에 저도 비슷한 작업을 거치며 약 80명이 넘는 국내외 수학자분들과 협업을 진행하였습니다. 그 과정에서 제가 받은 인상은 (1) 한 두개의 분야에 깊게 집중하는 인간과 달리 거의 모든 분야에서 평균 이상의 지식과 이해도를 가지는 언어모델의 특성 (2) 사람과 비교해 훨씬 더 빠르고 효율적으로 과거 문헌을 찾고 정리할 수 있다는 점에서 인공지능이 인간 수학자들이 가지지 못한, 그리고 현실적으로 가지기 매우 힘든 장점들을 가지고 있습니다. 나아가, \"요즘 인공지능이 수학과 박사들보다 똑똑하냐?\" 라는 질문에 한 교수님은 진담 반 농담 반, \"내 박사 학생보다는 확실히 똑똑한 것 같다\" 라고 대답하셨습니다. (외국인 교수님이였습니다)\n\n물론 아직 인공지능이 인간 수학자를 대체하기 위해서는 많은 제약 조건들이 존재합니다. 우선, 인공지능들이 “어려운 문제를 맞힐 가능성”이 생겼음은 분명하지만, 본인의 풀이가 옳다고 전달/설득하는 능력이 부재합니다. 인간 수학자들은 새로운 결과를 세미나와 피어 리뷰 등 사회적 과정을 거치며 신뢰를 얻습니다. 반대로 인공지능들은 이것이 불가능하기 때문에 검증을 인간 수학자에 의존해야만 하는 상황입니다. 또, 수학은 문제를 잘 푸는 것뿐 아니라 의미 있는 문제를 제안하는 것도 포괄합니다. 그런데 현재의 평가들은 대부분 “풀기”에 집중되어 있고, 올바른 연구 방향을 설정하고 문제를 만드는 능력은 상대적으로 덜 다뤄집니다. 이 부분은 앞으로 평가 설계에서 더 중요해질 것 같습니다.\n\n이러한 트렌드는 투자 흐름에도 반영되고 있습니다. 해외에서는 Harmonic (Series C $120M, 기업가치 $1.45B)과  Axiom Math (Seed $64M, 기업가치 $300M) 등 Lean을 활용해 검증을 자동화하고 AI 수학자를 만들려고 시도하는 스타트업들이 투자와 주목을 받고 있는 상황입니다. \n\nAI와 LLM을 활용해 수학/과학 연구를 가속화하려는 시도는 해외에서 매우 큰 주목을 받고 있습니다. 미국의 Genesis Mission은 Transformational AI Models Consortium (MODCON)을 통해 Quantum Algorithms, Combustion & Fluids, Critical Minerals, HEP & Cosmology, Specs-to-Silicon, Power Grid, 2D Quantum Magnets, Magnetic Fusion 등 수많은 영역에서 National Labs를 통해 AI를 활용한 과학 연구를 지원하는 상태입니다. (https://lnkd.in/g8bjEd9D) \n\n이제 단순히 좋은 인공지능 모델을 만들고 주목을 받는 순간은 지나가고 있습니다. 인공지능으로 얼마나 어렵고 의미 있는 문제를 풀어내는가? 가 중요해지만큼 특색 있는 산업을 가지고 있고, 잘 활용할 수 있는 국가들이 앞으로 더 앞서 나갈 수 있지 않을까 싶습니다.",
      "thumbnail": "https://static.licdn.com/aero-v1/sc/h/c45fy346jw096z9pbphyyhdz7",
      "siteName": "www.linkedin.com",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "adbeae18-7b08-4d00-aaac-e826c077c356",
      "createdAt": "2026-02-18T13:40:43.290Z",
      "updatedAt": "2026-02-18T13:40:43.290Z"
    },
    {
      "citeKey": "xiangyi2026",
      "url": "https://arxiv.org/abs/2602.12670",
      "type": "academic",
      "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
      "authors": [
        "Li",
        "Xiangyi",
        "Chen",
        "Wenbo",
        "Liu",
        "Yimin",
        "Zheng",
        "Shenghan",
        "Chen",
        "Xiaokun",
        "He",
        "Yifeng",
        "Li",
        "Yubo",
        "You",
        "Bingran",
        "Shen",
        "Haotian",
        "Sun",
        "Jiankai",
        "Wang",
        "Shuyi",
        "Zeng",
        "Qunhong",
        "Wang",
        "Di",
        "Zhao",
        "Xuandong",
        "Wang",
        "Yuanli",
        "Chaim",
        "Roey Ben",
        "Di",
        "Zonglin",
        "Gao",
        "Yipeng",
        "He",
        "Junwei",
        "He",
        "Yizhuo",
        "Jing",
        "Liqiang",
        "Kong",
        "Luyang",
        "Lan",
        "Xin",
        "Li",
        "Jiachen",
        "Li",
        "Songlin",
        "Li",
        "Yijiang",
        "Lin",
        "Yueqian",
        "Liu",
        "Xinyi",
        "Liu",
        "Xuanqing",
        "Lyu",
        "Haoran",
        "Ma",
        "Ze",
        "Wang",
        "Bowei",
        "Wang",
        "Runhui",
        "Wang",
        "Tianyu",
        "Ye",
        "Wengao",
        "Zhang",
        "Yue",
        "Xing",
        "Hanwen",
        "Xue",
        "Yiqi",
        "Dillmann",
        "Steven",
        "Lee",
        "Han-chung"
      ],
      "date": "2026-02-13",
      "description": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "93838763-9925-48e3-ba1c-78ea3a33c22c",
      "createdAt": "2026-02-18T13:41:24.945Z",
      "updatedAt": "2026-02-18T13:41:24.945Z"
    },
    {
      "citeKey": "taiwei2026",
      "url": "https://www.arxiv.org/abs/2602.13949",
      "type": "academic",
      "title": "Experiential Reinforcement Learning",
      "authors": [
        "Shi",
        "Taiwei",
        "Chen",
        "Sihao",
        "Jiang",
        "Bowen",
        "Song",
        "Linxin",
        "Yang",
        "Longqi",
        "Zhao",
        "Jieyu"
      ],
      "date": "2026-02-15",
      "description": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "2d1f0acc-bd47-48fc-8563-ec38af2dd43a",
      "createdAt": "2026-02-18T13:42:18.750Z",
      "updatedAt": "2026-02-18T13:42:18.750Z"
    },
    {
      "citeKey": "github2026e",
      "url": "https://github.com/Developer-Y/cs-video-courses",
      "type": "blog",
      "title": "GitHub - Developer-Y/cs-video-courses: List of Computer Science courses with video lectures.",
      "authors": [],
      "date": "",
      "description": "List of Computer Science courses with video lectures. - Developer-Y/cs-video-courses",
      "thumbnail": "https://opengraph.githubassets.com/eaccb4944088d075359080d67e9f16eef9c043534f296add3dbc9b5d32ee37d3/Developer-Y/cs-video-courses",
      "siteName": "GitHub",
      "tags": [
        "CS",
        "Lecture Video"
      ],
      "collections": [],
      "notes": "",
      "id": "544eaa0b-7f1d-43d0-b2a0-cc0f30f8e143",
      "createdAt": "2026-02-18T13:43:28.343Z",
      "updatedAt": "2026-02-18T13:43:28.343Z"
    },
    {
      "citeKey": "yaniv2025",
      "url": "https://arxiv.org/abs/2512.14982",
      "type": "academic",
      "title": "Prompt Repetition Improves Non-Reasoning LLMs",
      "authors": [
        "Leviathan",
        "Yaniv",
        "Kalman",
        "Matan",
        "Matias",
        "Yossi"
      ],
      "date": "2025-12-17",
      "description": "When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "8d9fb756-44aa-436b-af84-f977a681e823",
      "createdAt": "2026-02-18T13:44:53.936Z",
      "updatedAt": "2026-02-18T13:44:53.936Z"
    },
    {
      "citeKey": "wes2026",
      "url": "https://arxiv.org/abs/2601.04480",
      "type": "academic",
      "title": "When Models Manipulate Manifolds: The Geometry of a Counting Task",
      "authors": [
        "Gurnee",
        "Wes",
        "Ameisen",
        "Emmanuel",
        "Kauvar",
        "Isaac",
        "Tarng",
        "Julius",
        "Pearce",
        "Adam",
        "Olah",
        "Chris",
        "Batson",
        "Joshua"
      ],
      "date": "2026-01-08",
      "description": "Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count manifolds, attention heads twist these manifolds to estimate distance to the line boundary, and the decision to break the line is enabled by arranging estimates orthogonally to create a linear decision boundary. We validate our findings through causal interventions and discover visual illusions--character sequences that hijack the counting mechanism. Our work demonstrates the rich sensory processing of early layers, the intricacy of attention algorithms, and the importance of combining feature-based and geometric views of interpretability.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "53377758-b113-4aeb-b9ff-88852a8fd4d6",
      "createdAt": "2026-02-18T13:46:02.553Z",
      "updatedAt": "2026-02-18T13:46:02.553Z"
    },
    {
      "citeKey": "idan2026",
      "url": "https://arxiv.org/abs/2601.19897",
      "type": "academic",
      "title": "Self-Distillation Enables Continual Learning",
      "authors": [
        "Shenfeld",
        "Idan",
        "Damani",
        "Mehul",
        "Hübotter",
        "Jonas",
        "Agrawal",
        "Pulkit"
      ],
      "date": "2026-01-27",
      "description": "Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "1889e62d-b914-4416-a949-679c1f876252",
      "createdAt": "2026-02-18T13:48:20.876Z",
      "updatedAt": "2026-02-18T13:48:20.876Z"
    },
    {
      "citeKey": "ref2026",
      "url": "https://x.com/junfanzhu98/status/2023830685811962317?s=12&t=jFcYRHCjSynjxWae86HgVA",
      "type": "twitter",
      "title": "",
      "authors": [],
      "date": "",
      "description": "51. Equilibrium Matching: Paradigm Beyond Diffusion & Flow Model— AI Is Like Walking Down Mountain\nIn most modern generative models—Diffusion and Flow Matching especially—the sampling process is fundamentally non-equilibrium: the “rules” change at every time step. You start from pure random noise and gradually transform it into data, guided by a time-dependent differential equation. It’s like having a perfect GPS whose navigation rules keep updating as you move: at each moment, the system tells you how to step so that noise eventually becomes a realistic image.\nBut what if we could get rid of this dependence on time and non-equilibrium dynamics altogether?\nThat’s the motivation behind Equilibrium Matching (EqM): instead of a changing GPS, imagine a static terrain map + a magical compass that always points “uphill.” Once you’ve learned the landscape, you no longer need time as an explicit variable. You just follow the terrain.\n1. From Non-Equilibrium Dynamics to Equilibrium Matching\n1.1 Diffusion / Flow Matching: Time-Dependent Navigation\nDiffusion models and Flow Matching both rely on non-equilibrium processes:\nStart from fully random noise (a chaotic, high-entropy state).\nUse a time-dependent rule (SDE/ODE or a learned velocity field) to slowly morph noise into data.\nThe system is like a GPS that continuously updates its instructions as you move.\nThis is powerful but conceptually and computationally heavy: you must track a trajectory over time and learn how the dynamics change at each step.\n1.2 EqM: Static Map + Uphill Compass\nEqM proposes a different paradigm:\nInstead of a time-dependent process, we learn a static energy landscape over the data space.\nThink of a height map (terrain) plus a compass that always points uphill (toward higher energy).\nReal data should correspond to energy minima—the valleys on this landscape.\nEqM learns a time-independent gradient field (a vector field over space) that tells you the uphill direction everywhere.\nWhy uphill? Because if the model learns the gradient of an energy function (E(x)), then:\nThe gradient (\\nabla E(x)) points toward increasing energy (uphill).\nData points (x) should be energy minima, so the gradient must vanish at those points.\nAt training time, EqM learns how to climb uphill; at sampling time, you simply reverse direction and go downhill to reach the valleys where real data live.\nSo EqM replaces “dynamic guidance over time” with a static equilibrium structure: a fixed terrain you can descend on from any starting point.\n2. Manifold Hypothesis and the Internal Geometry of Data\nTo understand why diffusion-style models work so well—and why their smoothing behavior matters—we need the Manifold Hypothesis.\nManifold Hypothesis\nReal high-dimensional data (e.g., images with millions of pixels) do not fill the entire ambient space. Instead:\nData lie near a low-dimensional manifold embedded in high dimensions.\nThis manifold has its own geometry and structure.\nA generative model doesn’t need to memorize each training point; it can learn the manifold and then walk along it to produce new, realistic samples.\nIf the model captures the shape and geometry of this low-dimensional manifold, it can generate novel points that still look “real” because they stay on (or near) the same manifold.\n3. Diffusion Models, Score Functions, and Implicit Smoothing in Log Space\nDiffusion models typically have two phases:\nForward process (noising): Corrupt data with an SDE, gradually adding noise until they become near-pure Gaussian noise.\nReverse process (denoising): Learn to denoise step by step, effectively inverting the forward process.\nThe key object learned is the score function:\n[\ns\nθ\n(\nx\n,\nt\n)\n≈\n∇\nx\nlog\n⁡\np\nt\n(\nx\n)\n]\nIt tells you: given the current noise level, which direction in (x)-space increases the (log) probability density?\nIf the model learned the score function perfectly for the training data, it would risk memorizing the dataset—only producing tiny perturbations of existing examples. We want generalization, not rote memorization.\nImplicit Regularization via Smoothing in Log Domain\nTo avoid overfitting, diffusion training introduces an implicit regularization / smoothing effect, and crucially:\nThis smoothing happens in the log-density domain, not in raw probability.\nCompare to traditional smoothing in the data domain, such as Kernel Density Estimation (KDE):\nIn KDE, you put a Gaussian “bump” around each data point.\nBy the manifold hypothesis, real data occupy a thin manifold; off the manifold, probability should be near zero.\nSmoothing in data space is like taking a brush and smearing soil from the plateau (the manifold) into the abyss (off-manifold regions). You risk allocating non-negligible probability to regions that should be almost impossible.\nIn contrast, consider smoothing in the log domain:\nOff-manifold regions have probabilities near zero, so ( \\log p(x) \\to -\\infty ).\nWhen you smooth in log space, these “infinitely deep valleys” stay extremely low; you don’t easily drag significant probability mass into them.\nThe smoothing tends to occur within the plateau (on or near the manifold), not across a huge gap into the abyss.\nNow recall: the score function is the gradient of the log density. So:\nSmoothing the score is effectively smoothing the log-density.\nDiffusion training implicitly performs geometry-adaptive smoothing: It mainly smooths along the manifold, preserving structure. It avoids pushing probability mass into directions orthogonal to the manifold (off the cliff).\nThis geometric bias explains why diffusion models preserve intricate structures like perspective in landscapes or facial geometry: they respect the manifold’s geometry.\nBut:\nToo little smoothing → overfitting / memorization.\nToo much smoothing → you wash out the subtle but meaningful geometry of the manifold.\nThe choice of smoothing mechanism shapes the model’s geometric bias—like choosing a particular shape of “glasses” through which the model sees and connects discrete data points.\n4. EqM: A New Equilibrium Paradigm Beyond Non-Equilibrium Flows\nEqM aims to build a generative framework fundamentally rooted in equilibrium:\nNo explicit time steps.\nNo time-dependent target.\nInstead: a static map + eternal compass.\nThere is a related classical idea: Energy-Based Models (EBMs), which also try to learn an energy landscape. But EBM training is notoriously unstable and difficult to scale.\nEqM’s key contribution is a carefully designed training objective that:\nConnects to energy landscapes like EBMs, and\nRetains the stability and scalability of flow-based/diffusion approaches.\n5. Flow Matching vs EqM: Opposite Gradient Directions\nIn Flow Matching, the model learns a velocity field that transports noise to data:\nYou usually mix a data sample (x) with noise (\\epsilon), and define a target direction like: . i.e., a direction from noise to data.\n[\nv\nFM\n∝\nx\n−\nϵ\n]\nIn EqM, the target is ingeniously reversed:\nEqM defines its training target gradient as: [ g_{\\text{EqM}} \\propto \\epsilon - x ]. i.e., a direction from data to noise, the exact opposite of Flow Matching.\nWhy? Because EqM is designed from an energy perspective:\nReal data (x) should correspond to energy minima.\nThe gradient of the energy function always points toward increasing energy (uphill).\nNear a real data point (x), the uphill direction must point away from (x) (toward higher-energy regions, e.g., noisy states like (\\epsilon)).\nTherefore, EqM’s target gradient around (x) must be oriented from data toward noise.\nThis flips the usual intuition:\nFlow Matching: “How do I flow from noise to data?”\nEqM: “What gradient field makes data points become valleys in an energy landscape, where the uphill direction is toward noise?”\nLearning this uphill field means that at sampling time we can just go downhill.\n6. Two Key Design Choices in EqM\n6.1 Mixing Data and Noise via a Random Interpolation Factor γ\nEqM constructs mixed states between data (x) and noise (\\epsilon) using a random interpolation factor (\\gamma):\n[\nz\nγ\n=\nγ\nx\n+\n(\n1\n−\nγ\n)\nϵ\n]\nCrucially:\n(\\gamma) is not given as input to the model.\nThe model (F) learns a single, unified, static gradient field that works for all mixing levels.\nIn other words, (F) doesn’t learn a time-dependent field; it learns the geometry of the terrain itself, independent of any “time” or “schedule”.\nThis is how EqM “escapes” dependence on dynamic variables and moves to a pure equilibrium view.\n6.2 Amplitude Controller (c(\\gamma)): Enforcing Valleys at Data Points\nEqM uses a scalar amplitude controller (c(\\gamma)), which modulates the magnitude of the target gradient:\n[\ntarget gradient\n∝\nc\n(\nγ\n)\n⋅\n(\nϵ\n−\nx\n)\n]\nInterpretation:\n(c(\\gamma)) is like a light dimmer that controls how strong the uphill signal should be at different mixing levels.\nThe critical constraint is: [ c(1) = 0 ]. When (\\gamma = 1), the mixed point is exactly the data sample (x).\nThis enforces that in the final learned energy landscape, all real data points (x) must satisfy:\nGradient = 0 at (x).\nSo data points become stable equilibrium points—true valleys of the energy function.\nA particularly effective choice is a truncated decay schedule:\nFar from the valley (heavily mixed with noise), the gradient magnitude is a relatively large constant, giving strong “downhill force” during sampling.\nNear the valley, the gradient magnitude rapidly decays to 0, allowing the optimization to settle smoothly instead of overshooting.\nOverall, the EqM training objective combines:\nOpposite target gradient direction ((\\epsilon - x) instead of (x - \\epsilon)).\nA model that does not depend on dynamic variables (same field for all (\\gamma)).\nA precise zero-gradient constraint at data points via (c(1) = 0).\nThe result: the model learns a static, well-behaved energy landscape over data space.\n7. Sampling with EqM: Optimization-Based Generation\nOnce EqM has learned the energy landscape, how do we generate samples?\nInstead of solving an SDE/ODE, EqM uses optimization-based sampling:\nStart from an arbitrary point (e.g., random noise).\nPerform gradient descent on the learned energy function: Move downhill repeatedly until you reach a valley.\nThis is classic optimization rather than numerical integration of a time-dependent flow. It brings several advantages:\nRobustness to Step Size: Gradient descent is more forgiving: you don’t need a very finely tuned schedule of step sizes to maintain sample quality.\nUse of Advanced Optimizers. You can plug in powerful optimizers like Nesterov Accelerated Gradient (NAG): Uses momentum to smooth updates. Takes a look-ahead step before computing the gradient, which can help (Converge faster / Jump over shallow local minima and find better valleys.)\nAdaptive Compute (Adaptive Number of Steps). You don’t need a fixed number of iterations. You can set a stopping condition, e.g., when gradient norm falls below a threshold. This means: Simple samples (easy terrain) converge quickly with fewer steps or Harder samples (rugged terrain) naturally consume more compute. Overall, the system automatically allocates compute per sample based on landscape difficulty, improving efficiency.\nSo EqM reframes generation as energy optimization rather than trajectory simulation.\n8. Unique Properties of EqM\nEqM’s equilibrium structure yields several distinctive advantages.\n8.1 Partially Noised Image Denoising\nTraditional diffusion models are typically trained to follow a specific noise schedule. If you drop them into a partially noised intermediate state that doesn’t lie on that schedule (e.g., arbitrary noise injection), they can struggle or behave unpredictably.\nEqM, however, learns a global, static energy landscape:\nNo matter where you start—pure noise, partially corrupted image, or in-between—\nThe same gradient field tells you how to descend toward the nearest valley.\nThis makes EqM naturally good at partial denoising and handling arbitrary noise levels without retraining or schedule engineering.\n8.2 Built-in Out-of-Distribution (OOD) Detection\nEqM has a natural out-of-distribution detection mechanism:\nIn-distribution (ID) data lie in low-energy valleys.\nOOD data lie on the hillsides or high-energy regions.\nBy simply inspecting the energy value estimated by the model, you can tell whether a sample is “normal” or “weird.”\nNo extra classifier is required. This built-in OOD ability is especially valuable for safety and reliability: EqM can flag suspicious inputs by energy level alone.\n8.3 Simple Composition of Concepts\nEqM makes compositional generation conceptually straightforward.\nSuppose you have:\nAn EqM model for pandas (with its own energy landscape and gradient field).\nAn EqM model for bamboo.\nTo generate “panda eating bamboo,” you can:\nAt each step, add the gradient vectors from the two models: Compute gradients from the panda model and from the bamboo model. Sum them to get a combined “force”.\nThen perform gradient descent along the negative of that combined gradient.\nThe system effectively descends a combined energy landscape representing the joint concept—without complex hacks or auxiliary tricks typical in diffusion pipelines.\nThis vectorial composition of gradients is intuitive and elegant in the energy-based view.\n9. EqM as a Bridge Between Diffusion and Energy-Based Models\nIn summary, EqM occupies a sweet spot between:\nThe power and stability of diffusion / flow models, and\nThe elegant equilibrium semantics of Energy-Based Models.\nIt:\nRetains the soul of EBMs (learning a static energy landscape, equilibrium reasoning).\nInherits the body of flow models (scalable, stable training and high-quality generation).\n10. Looking Ahead\nEqM also opens several exciting research directions:\nIncorporating second-order information (curvature of the energy landscape) to enable: More advanced optimizers. Faster convergence. Higher sample quality and better controllability.\nViewing generation as direct manipulation of the energy landscape itself: Instead of only searching for minima, we could edit the learned energy function to: Shape where valleys lie. Sculpt constraints or preferences. Exert fine-grained control over the generative process.\nBy reframing generative modeling as equilibrium energy optimization, EqM doesn’t just propose a new algorithm—it suggests a new way to understand and control how generative models create.\n11. Refernces\n[1] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models. Runqian Wang, Yilun Du. https://arxiv.org/abs/2510.02300\n[2] ⛰️ 51. Equilibrium Matching: Paradigm Beyond Diffusion & Flow Models — GenAI Is Just Like Walking Down a Mountain https://www.linkedin.com/pulse/51-equilibrium-matching-diffusion-models-why-generative-ai-like-rqdgc",
      "thumbnail": "",
      "siteName": "X (formerly Twitter)",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "78a135b6-4cc0-4938-8295-bce468b337fc",
      "createdAt": "2026-02-18T13:49:22.232Z",
      "updatedAt": "2026-02-18T13:49:22.232Z"
    },
    {
      "citeKey": "jonathan2026",
      "url": "https://arxiv.org/abs/2602.14853",
      "type": "academic",
      "title": "BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations",
      "authors": [
        "Gorard",
        "Jonathan",
        "Hakim",
        "Ammar",
        "Juno",
        "James"
      ],
      "date": "2026-02-16",
      "description": "The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "3c5eb139-f290-4b83-8b78-7eaee9f16d3b",
      "createdAt": "2026-02-18T13:49:52.113Z",
      "updatedAt": "2026-02-18T13:49:52.113Z"
    },
    {
      "citeKey": "davide2026",
      "url": "https://arxiv.org/abs/2602.03545",
      "type": "academic",
      "title": "Persona Generators: Generating Diverse Synthetic Personas at Scale",
      "authors": [
        "Paglieri",
        "Davide",
        "Cross",
        "Logan",
        "Cunningham",
        "William A.",
        "Leibo",
        "Joel Z.",
        "Vezhnevets",
        "Alexander Sasha"
      ],
      "date": "2026-02-03",
      "description": "Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "0ca87902-232d-4543-be5f-2e2fbb040fe5",
      "createdAt": "2026-02-18T13:50:20.292Z",
      "updatedAt": "2026-02-18T13:50:20.292Z"
    },
    {
      "citeKey": "performance2026",
      "url": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/",
      "type": "blog",
      "title": "Performance Engineering of Software Systems | Electrical Engineering and Computer Science | MIT OpenCourseWare",
      "authors": [],
      "date": "",
      "description": "6.172 is an 18-unit class that provides a hands-on, project-based introduction to building scalable and high-performance software systems. Topics include performance analysis, algorithmic techniques for high performance, instruction-level optimizations, caching optimizations, parallel programming, and building scalable systems. The course programming language is C.",
      "thumbnail": "https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/2ea5b1550197db095fcb82df55203675_6-172f18.jpg",
      "siteName": "MIT OpenCourseWare",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "d4328040-6e99-4f6e-a06b-f13b318ceec2",
      "createdAt": "2026-02-20T01:19:40.289Z",
      "updatedAt": "2026-02-20T01:19:40.289Z"
    },
    {
      "citeKey": "weilin2026",
      "url": "https://arxiv.org/abs/2602.13517",
      "type": "academic",
      "title": "Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens",
      "authors": [
        "Chen",
        "Wei-Lin",
        "Peng",
        "Liqian",
        "Tan",
        "Tian",
        "Zhao",
        "Chao",
        "Chen",
        "Blake JianHang",
        "Lin",
        "Ziqian",
        "Go",
        "Alec",
        "Meng",
        "Yu"
      ],
      "date": "2026/02/13",
      "description": "Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal &#34;overthinking,&#34; leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.\n\n\n1. 문제의식: reasoning 성능을 높이기 위해 CoT 길이를 늘리는 방식이 일반적이지만, 단순 토큰 길이는 실제 “생각 effort”를 제대로 반영하지 못한다는 점을 지적함. 길어도 얕은 계산일 수 있고, 짧아도 깊은 계산일 수 있음. \n2. 핵심 가설: reasoning effort는 “얼마나 오래 출력했는가”가 아니라 “토큰이 내부 네트워크 깊이에서 얼마나 많이 수정·재계산되었는가”로 봐야 한다고 정의함. 즉 depth-wise computation을 측정해야 함. \n3. 이를 위해 deep-thinking tokens 개념을 제안함. 토큰 생성 과정에서 예측이 초기 레이어에서 안정되는 경우는 얕은 사고, 깊은 레이어까지 계속 수정되는 경우는 깊은 사고로 간주함. \n4. 구체적으로 deep-thinking ratio(DTR)를 정의함. 각 토큰이 네트워크 깊이 방향으로 얼마나 많은 업데이트를 거쳤는지를 측정해 inference-time thinking effort의 직접 지표로 사용함. \n5. 기존 지표(출력 길이, token count, latency)는 surface-level proxy라 reasoning effort와 상관이 약하거나 왜곡될 수 있다고 분석함. 특히 긴 CoT가 항상 깊은 reasoning을 의미하지 않는다는 점을 실험적으로 보여주려 함. \n6. 프레임워크는 토큰 생성 시 각 레이어에서의 logits 변화를 추적해 “깊이 방향 수정량”을 계산하고, 이를 토큰별 deep-thinking score로 집계하는 방식임. \n7. 직관적으로는\n- early layer에서 prediction이 바로 고정되면 shallow thinking\n- 깊은 레이어까지 계속 재조정되면 deep thinking으로 분류됨. \n8. 이 분석은 CoT reasoning 중 실제로 “생각이 필요한 토큰”과 단순 연결/형식 토큰을 분리할 수 있게 해줌. reasoning trace 전체를 동일하게 보지 않고 effort가 집중되는 구간을 찾는 목적임. \n9. 논문은 reasoning 모델의 compute scaling이 단순 length scaling이 아니라 depth-aware scaling으로 이해되어야 한다고 주장함. 즉 test-time compute를 “얼마나 길게”가 아니라 “얼마나 깊게” 쓰느냐의 문제로 재정의함. \n10. 이 지표를 사용하면 어떤 reasoning 단계가 실제 계산 부담이 큰지, 모델이 언제 진짜로 고민하는지, 길이만 늘린 CoT와 깊은 reasoning CoT의 차이를 분리해서 분석할 수 있다고 설명함. \n11. 더 나아가 deep-thinking token 분석은 inference 최적화에도 연결됨. 얕은 토큰은 계산을 줄이고, 깊은 토큰에는 compute를 더 할당하는 adaptive inference 설계로 이어질 수 있다고 제안함. \n12. 전체 메시지는 reasoning scaling을 length 중심에서 depth 중심으로 재정의해야 한다는 것. CoT 길이 증가만으로는 reasoning effort를 설명할 수 없으며, 토큰 내부 계산 깊이를 측정하는 새로운 분석 축이 필요하다는 주장임.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "ad15123e-239b-4ce6-915d-61934762c0b9",
      "createdAt": "2026-02-20T02:33:43.447Z",
      "updatedAt": "2026-02-20T02:33:43.447Z"
    },
    {
      "citeKey": "weilin2026a",
      "url": "https://arxiv.org/abs/2602.13517",
      "type": "academic",
      "title": "Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens",
      "authors": [
        "Chen",
        "Wei-Lin",
        "Peng",
        "Liqian",
        "Tan",
        "Tian",
        "Zhao",
        "Chao",
        "Chen",
        "Blake JianHang",
        "Lin",
        "Ziqian",
        "Go",
        "Alec",
        "Meng",
        "Yu"
      ],
      "date": "2026/02/13",
      "description": "Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal &#34;overthinking,&#34; leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.",
      "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
      "siteName": "arXiv.org",
      "tags": [],
      "collections": [],
      "notes": "",
      "id": "6fd38154-4682-4dbb-8b74-ec0c9d62f8a3",
      "createdAt": "2026-02-22T01:40:33.406Z",
      "updatedAt": "2026-02-22T01:40:33.406Z"
    }
  ],
  "collections": [
    "Agent"
  ],
  "tags": []
}