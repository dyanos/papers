[
  {
    "citeKey": "weixun2025",
    "url": "https://arxiv.org/abs/2512.24873",
    "type": "academic",
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "authors": [
      "Wang",
      "Weixun",
      "Xu",
      "XiaoXiao",
      "An",
      "Wanhe",
      "Dai",
      "Fangwen",
      "Gao",
      "Wei",
      "He",
      "Yancheng",
      "Huang",
      "Ju",
      "Ji",
      "Qiang",
      "Jin",
      "Hanqi",
      "Li",
      "Xiaoyang",
      "Li",
      "Yang",
      "Li",
      "Zhongwen",
      "Lin",
      "Shirong",
      "Liu",
      "Jiashun",
      "Liu",
      "Zenan",
      "Luo",
      "Tao",
      "Muhtar",
      "Dilxat",
      "Qu",
      "Yuanbin",
      "Shi",
      "Jiaqiang",
      "Sun",
      "Qinghui",
      "Tan",
      "Yingshui",
      "Tang",
      "Hao",
      "Wang",
      "Runze",
      "Wang",
      "Yi",
      "Wang",
      "Zhaoguo",
      "Wu",
      "Yanan",
      "Xiong",
      "Shaopan",
      "Xu",
      "Binchen",
      "Xu",
      "Xander",
      "Xu",
      "Yuchi",
      "Zhang",
      "Qipeng",
      "Zhang",
      "Xixia",
      "Zhao",
      "Haizhou",
      "Zhao",
      "Jie",
      "Zhao",
      "Shuaibing",
      "Zheng",
      "Baihui",
      "Zheng",
      "Jianhui",
      "Zheng",
      "Suhang",
      "Zhu",
      "Yanni",
      "Cai",
      "Mengze",
      "Cao",
      "Kerui",
      "Chen",
      "Xitong",
      "Dai",
      "Yue",
      "Du",
      "Lifan",
      "Feng",
      "Tao",
      "He",
      "Tao",
      "Hu",
      "Jin",
      "Hu",
      "Yijie",
      "Jiang",
      "Ziyu",
      "Li",
      "Cheng",
      "Li",
      "Xiang",
      "Liang",
      "Jing",
      "Lin",
      "Xin",
      "Liu",
      "Chonghuan",
      "Liu",
      "ZhenDong",
      "Lv",
      "Zhiqiang",
      "Mi",
      "Haodong",
      "Mo",
      "Yanhu",
      "Ni",
      "Junjia",
      "Pei",
      "Shixin",
      "Shen",
      "Jingyu",
      "Song",
      "XiaoShuai",
      "Wang",
      "Cecilia",
      "Wang",
      "Chaofan",
      "Wang",
      "Kangyu",
      "Wang",
      "Pei",
      "Wang",
      "Tao",
      "Wang",
      "Wei",
      "Xiao",
      "Ke",
      "Xu",
      "Mingyu",
      "Xu",
      "Tiange",
      "Ya",
      "Nan",
      "Yang",
      "Siran",
      "Ye",
      "Jianan",
      "Zang",
      "Yaxing",
      "Zhang",
      "Duo",
      "Zhang",
      "Junbo",
      "Zheng",
      "Boren",
      "Deng",
      "Wanxi",
      "Pan",
      "Ling",
      "Qu",
      "Lin",
      "Su",
      "Wenbo",
      "Wang",
      "Jiamang",
      "Wang",
      "Wei",
      "Wei",
      "Hu",
      "Wu",
      "Minggang",
      "Yu",
      "Cheng",
      "Zhao",
      "Bing",
      "Zheng",
      "Zhicheng",
      "Zheng",
      "Bo"
    ],
    "date": "2025/12/31",
    "description": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agentic model. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME, an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-Perceptive Agentic Policy Optimization (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of ALE.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [],
    "collections": [],
    "notes": "1\\. 문제: agentic crafting은 한 번 답하고 끝이 아니라, 여러 턴 동안 tool을 쓰고 결과를 보고 산출물을 계속 고치며 목표를 만족해야 해서, 데이터 생성·학습·실행 인프라가 약하면 연구가 진행되기 어렵고 파이프라인 병목에서 막힌다고 봄. \n\n2\\. Agent 학습용 풀스택으로 제안함: ROCK이 샌드박스 환경을 관리하며 대규모 trajectory를 만들고, ROLL이 그 trajectory로 post-training 최적화(RL/SFT 등)를 돌리며, iFlow CLI가 실제 실행 시 컨텍스트 구성·툴 라우팅·로그 수집 같은 “에이전트 운영 레이어”를 담당하는 식으로 역할을 분리함. \n\n3\\. ROME(ROME is Obviously an Agentic Model)이라는 모델을 학습한 결과물로 제시하며, 100만+ trajectory로 훈련했다고 밝힘. 여기서 요지는 모델만이 아니라 “환경에서 상호작용→로그/검증→학습으로 재투입”되는 루프가 표준화돼야 성능이 스케일된다는 주장임. \n\n4\\. 데이터는 단순한 정적 데모만 모으는 게 아니라, 멀티턴에서 자주 생기는 실패(잘못된 툴 호출, 중간 산출물 품질 저하, 되돌리기/재시도 등)를 포함해 “복잡 행동”이 나오도록 구성하는 data composition 프로토콜을 강조함. 즉 성공 궤적만이 아니라 실패와 회복까지 학습 신호로 쓰는 쪽임. \n\n5\\. IPA(Interaction-Perceptive Agentic Policy Optimization)의 직관은 토큰 단위로 전부 같은 크레딧을 주지 말고, 실제 에이전트 관점에서 의미 있는 단위(예: 검색→결과 읽기→클릭 같은 한 덩어리 상호작용)를 semantic interaction chunk로 묶어서 그 덩어리 단위로 credit assignment를 하자는 것임. 긴 horizon에서 “어느 턴/행동이 이득이었는지”가 토큰 레벨보다 덜 흐려져 학습이 안정화된다고 설명함. \n\n6\\. 이 접근은 멀티턴 에이전트에서 흔한 문제(보상이 끝에만 있고, 중간에는 noisy한 관측/툴 출력이 많아서 토큰 레벨 학습이 흔들리는 문제)를 “상호작용 이벤트 중심”으로 재정렬해 주는 효과를 노림. 그래서 iFlow/ROCK에서 기록되는 로그가 곧 chunk 경계와 정렬된 학습 데이터가 되게 설계되는 그림임. \n\n7\\. 평가 쪽에서는 Terminal Bench Pro를 소개하면서 스케일과 contamination control을 개선했다고 주장하고, ROME을 SWE-bench Verified, Terminal Bench 계열 등에서 평가해 ALE+IPA 스택이 성능에 기여한다고 결론냄. \n\n8\\. 공개된 모델 카드 요약 기준으로, ROME은 (MoE 30B, active 3B 설정이라고 소개되며) Terminal-Bench 2.0 24.72%, SWE-bench Verified 57.40%를 보고함.",
    "id": "7fd07709-732f-476a-b978-b1cb9121ba32",
    "createdAt": "2026-02-13T11:57:54.989Z",
    "updatedAt": "2026-02-13T11:58:22.051Z"
  },
  {
    "citeKey": "zhaoyang2026",
    "url": "https://arxiv.org/abs/2602.10090",
    "type": "academic",
    "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
    "authors": [
      "Wang",
      "Zhaoyang",
      "Xu",
      "Canwen",
      "Liu",
      "Boyi",
      "Wang",
      "Yite",
      "Han",
      "Siwei",
      "Yao",
      "Zhewei",
      "Yao",
      "Huaxiu",
      "He",
      "Yuxiong"
    ],
    "date": "2026/02/10",
    "description": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [
      "Agent",
      "World Model"
    ],
    "collections": [],
    "notes": "1\\. 문제: agent RL을 크게 키우려면 환경이 많이 필요하지만, 현실 환경은 느리고 비싸며, LLM로 “말로만” 시뮬레이션한 환경은 상태 전이가 들쭉날쭉해서 학습 신호가 불안정하다는 점에서 출발함.\n\n2\\. 제안: Agent World Model(AWM)은 환경을 “텍스트 역할극”이 아니라 코드+DB로 합성하는 파이프라인임. 1,000개 일상 시나리오 환경을 만들고, 평균 35개 도구를 가진 tool-use 환경으로 노출함. 상태는 SQLite DB가 들고, 행동은 API 호출, 관측은 API 응답으로 정의됨.\n\n3\\. 환경을 만드는 절차(직관): 시나리오 하나를 정하면 그 시나리오에서 유저가 할 법한 “해야 할 일(task)” 10개를 먼저 만들고, 그 일을 가능하게 하는 최소한의 DB 스키마/초기데이터를 만든 뒤, 그 DB를 조작하는 도구(API)들을 생성해서 “실제로 실행되는 서비스”처럼 굴리게 함.\n\n4\\. 구체 파이프라인(생성물 기준): (scenario) → (tasks) → (DB schema+sample data) → (API spec) → (환경 서버 코드: FastAPI + MCP 인터페이스) → (검증 코드 verifiers) 순으로 뽑음. 이때 verifiers는 “DB 상태가 목표대로 바뀌었는지”를 코드로 검사해서 reward를 안정적으로 주기 위한 장치임.\n\n5\\. 학습이 쉬워지는 포인트: 환경이 완전 실행 가능(executable)하고 내부 DB state를 접근 가능하게 설계돼서, “LLM-judge 말판정” 대신 “DB 상태 검사”로 보상을 만들 수 있음. 그래서 reward hacking/판정 불안정이 줄고, rollout도 현실 환경보다 훨씬 싸게 많이 찍을 수 있다는 논리임.\n\n6\\. 실험 구성: 벤치마크 전용 환경에서만 훈련하는 대신, AWM으로 만든 합성 환경에서만 RL로 훈련한 뒤, 서로 다른 3개 벤치마크로 OOD 일반화를 평가하는 설정을 강조함.\n\n7\\. 성능(요약 수치): 공개 요약 기준으로 BFCLv3에서 8B 모델이 53.83→65.94(+12.11)로 상승, MCP-Universe에서 8B가 6.70→11.17로 상승, τ²-bench에서는 14B가 Pass@1 39.03에 도달했다고 보고됨.\n\n8\\. 결과가 말하는 바: “벤치마크 맞춤 환경”이 아니라 “일반 합성 환경 묶음”에서 RL을 돌려도, 오히려 도구사용/멀티턴 상호작용 능력이 범용적으로 올라가서 OOD 벤치에서 이득이 난다는 메시지임.",
    "id": "91320af7-31f1-49cb-af74-7dfe161070b8",
    "createdAt": "2026-02-13T11:59:46.977Z",
    "updatedAt": "2026-02-13T11:59:59.971Z"
  },
  {
    "citeKey": "yujiong2026",
    "url": "https://arxiv.org/abs/2602.12984",
    "type": "academic",
    "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
    "authors": [
      "Shen",
      "Yujiong",
      "Yang",
      "Yajie",
      "Xi",
      "Zhiheng",
      "Hu",
      "Binze",
      "Sha",
      "Huayu",
      "Zhang",
      "Jiazheng",
      "Peng",
      "Qiyuan",
      "Shang",
      "Junlin",
      "Huang",
      "Jixuan",
      "Fan",
      "Yutao",
      "Tong",
      "Jingqi",
      "Dou",
      "Shihan",
      "Zhang",
      "Ming",
      "Bai",
      "Lei",
      "Yin",
      "Zhenfei",
      "Gui",
      "Tao",
      "Ma",
      "Xingjun",
      "Zhang",
      "Qi",
      "Huang",
      "Xuanjing",
      "Jiang",
      "Yu-Gang"
    ],
    "date": "2026/02/13",
    "description": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [
      "Gym",
      "Scientic Agent"
    ],
    "collections": [],
    "notes": "이 논문은 **SciAgentGym**이라는 새로운 벤치마크 환경을 소개합니다. 핵심 요점은 다음과 같습니다:\n\n### 주요 내용 요약\n\n- **문제의식**: 기존 과학 벤치마크는 단순한 질의응답 중심이라 실제 과학 연구에서 필요한 **도구 활용 기반의 다단계 추론**을 반영하지 못함.\n\n- **SciAgentGym**: 물리학, 화학, 생물학, 재료과학 등 4개 분야에서 **1,780개 도메인 특화 도구**를 통합한 대규모 상호작용 환경. 파일 시스템, 데이터베이스, 파이썬 인터프리터 등 실행 인프라 포함.\n\n- **SciAgentBench**: 259개 과제와 1,134개 하위 질문으로 구성된 평가 세트. 난이도를 L1(단순)\\~L3(장기적 복잡)로 구분해 모델의 도구 활용 능력을 정밀하게 측정.\n\n- **주요 발견**: 최신 모델(GPT-5 포함)도 **복잡한 장기적 과학 워크플로우**에서는 성능이 급격히 저하됨. 예: GPT-5는 단순 과제(L1)에서 60.6% 성공률이지만, 복잡 과제(L3)에서는 30.9%로 하락.\n\n- **SciForge**: 도구 간 의존성을 그래프 구조로 모델링해 **논리적 실행 경로 기반 학습 데이터**를 합성하는 방법. 이를 통해 모델이 오류 복구와 다단계 추론을 더 잘 수행하도록 개선.\n\n- **성과**: SciAgent-8B 모델은 훨씬 큰 모델(Qwen3-VL-235B-Instruct)보다 뛰어난 성능을 보였으며, **분야 간 전이 학습 효과**도 확인됨.\n\n- **결론**: 도구 활용 기반의 학습과 평가가 과학적 문제 해결에 필수적이며, SciAgentGym과 SciForge는 차세대 과학 AI 에이전트 개발의 토대를 마련함.",
    "id": "115d3924-ab41-41d1-99b0-e1150f0662f9",
    "createdAt": "2026-02-17T11:17:23.445Z",
    "updatedAt": "2026-02-17T11:20:59.761Z"
  },
  {
    "citeKey": "yuzheng2025",
    "url": "https://arxiv.org/abs/2510.08191v1",
    "type": "academic",
    "title": "Training-Free Group Relative Policy Optimization",
    "authors": [
      "Cai",
      "Yuzheng",
      "Cai",
      "Siqi",
      "Shi",
      "Yuchen",
      "Xu",
      "Zihan",
      "Chen",
      "Lichao",
      "Qin",
      "Yulei",
      "Tan",
      "Xiaoyu",
      "Li",
      "Gang",
      "Li",
      "Zongyi",
      "Lin",
      "Haojia",
      "Mao",
      "Yong",
      "Li",
      "Ke",
      "Sun",
      "Xing"
    ],
    "date": "2025/10/09",
    "description": "Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [
      "GRPO"
    ],
    "collections": [],
    "notes": "[\\[논문 리뷰\\] Training-Free Group Relative Policy Optimization | SuanLab Blog | SuanLab](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/)\n\nLLM의 성능을 향상시키기 위한 기존 연구는 주로 강화 학습과 지도 학습을 결합한 방법론에 초점을 맞추고 있습니다. 대표적인 연구로는 다음과 같은 것들이 있습니다:\n\n1. **Supervised Fine-Tuning (SFT)**: LLM의 성능을 개선하기 위해 대규모의 레이블링된 데이터셋을 사용하여 모델을 미세 조정하는 방법입니다. 그러나 이는 데이터가 많이 필요하고, 과적합의 위험이 있습니다. 최근에는 LoRA(Low-Rank Adaptation)와 같은 파라미터 효율적인 파인튜닝(PEFT) 기법들이 SFT의 단점을 보완하기 위해 연구되고 있습니다.\n\n2. **Reinforcement Learning with Human Feedback (RLHF)**: 인간의 피드백을 활용하여 LLM의 출력을 향상시키는 방법입니다. 이는 인간의 주관적인 평가를 반영할 수 있지만, 피드백 수집에 많은 비용과 시간이 소요됩니다. DPO(Direct Preference Optimization)와 같은 알고리즘은 RLHF의 복잡성을 줄이고 안정성을 높이는 방향으로 발전하고 있습니다.\n\n3. **Group Relative Policy Optimization (GRPO)**: 강화 학습에서 사용되는 정책 최적화 방법 중 하나로, 그룹 내에서 상대적인 성능을 비교하여 정책을 개선합니다. 그러나 이는 여전히 매개변수 업데이트가 필요합니다. 기존 GRPO는 정책 경사(policy gradient) 방법을 사용하여 정책을 업데이트하지만, Training-Free GRPO는 이러한 업데이트 과정을 생략합니다.\n\n4. **Experience Replay**: 과거의 경험을 저장하고 재사용하여 학습 효율성을 높이는 방법입니다. 이는 데이터 활용도를 높일 수 있지만, LLM의 경우 경험을 저장하고 재사용하는 것이 쉽지 않습니다. LLM의 경우, 경험을 저장하는 대신 프롬프트 엔지니어링을 통해 유사한 효과를 얻을 수 있습니다.\n\n5. **Prompt Engineering**: LLM의 성능을 향상시키기 위해 프롬프트를 조정하는 방법입니다. 이는 모델의 출력에 큰 영향을 미칠 수 있지만, 최적의 프롬프트를 찾는 것이 어렵습니다. AutoPrompt와 같은 자동 프롬프트 생성 기법은 최적의 프롬프트를 찾는 과정을 자동화하려는 시도입니다.\n\nTraining-Free GRPO는 이러한 기존 방법론과 차별화됩니다. 이 방법론은 매개변수 업데이트 없이도 성능을 향상시킬 수 있으며, 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정합니다. 이는 데이터 부족 문제를 해결하고, 과적합을 방지할 수 있는 장점이 있습니다. 또한, 기존 방법들이 파인튜닝 과정에서 발생할 수 있는 catastrophic forgetting 문제를 완화할 수 있습니다.\n\n**연구 방법론특징차별점**SFT대규모 레이블링 데이터 필요데이터 효율성 부족, 과적합 위험RLHF인간 피드백 활용비용과 시간 소요, 주관성 개입GRPO그룹 내 상대적 성능 비교매개변수 업데이트 필요, 연산 비용 증가Experience Replay경험 저장 및 재사용LLM 적용 어려움, 메모리 부담Prompt Engineering프롬프트 조정최적 프롬프트 찾기 어려움, 휴리스틱 의존**Training-Free GRPO**파라미터 업데이트 불필요, 토큰 사전 학습데이터 효율성 높음, 과적합 방지, 연산 비용 절감\n\n## [**핵심 기여**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%B5%EC%8B%AC-%EA%B8%B0%EC%97%AC)\n\n1. **Training-Free GRPO 제안**: 매개변수 업데이트 없이 LLM의 성능을 향상시키는 새로운 방법론을 제안합니다. 이는 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정합니다. 이는 LLM의 implicit knowledge를 효과적으로 활용하는 새로운 접근 방식입니다.\n\n2. **데이터 효율성 개선**: 소수의 훈련 샘플만으로도 성능을 향상시킬 수 있는 데이터 효율적인 방법을 제시합니다. 이는 데이터 부족 문제를 해결하는 데 기여합니다. 특히 few-shot 또는 zero-shot learning 환경에서 유용합니다.\n\n3. **다양한 도메인에서의 성능 향상**: 수학적 추론, 웹 검색 등 다양한 작업에서 효과적인 성능 향상을 보였습니다. 이는 LLM 에이전트의 활용 가능성을 넓히는 데 기여합니다. 이는 Training-Free GRPO가 특정 도메인에 국한되지 않고 일반적인 문제 해결 능력 향상에 기여함을 시사합니다.\n\n4. **과적합 방지**: 경험적 지식을 활용하여 과적합을 방지할 수 있는 방법을 제시합니다. 이는 모델의 일반화 성능을 향상시키는 데 기여합니다. 토큰 사전은 일종의 regularization 효과를 제공하여 과적합을 방지합니다.\n\n## [**제안 방법론**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%A0%9C%EC%95%88-%EB%B0%A9%EB%B2%95%EB%A1%A0)\n\nTraining-Free GRPO는 매개변수 업데이트 없이도 LLM의 성능을 향상시키는 방법론입니다. 이 방법론의 핵심 아이디어는 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정하는 것입니다. 이는 데이터 부족 문제를 해결하고, 과적합을 방지할 수 있는 장점이 있습니다. Training-Free GRPO는 LLM의 출력 분포를 미세하게 조정하여 정답에 가까운 출력을 유도합니다.\n\n### [**모델 아키텍처**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EB%AA%A8%EB%8D%B8-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98)\n\nTraining-Free GRPO는 LLM의 출력 분포를 조정하기 위해 경험적 지식을 토큰 사전으로 학습합니다. 이 토큰 사전은 긍정적인 피드백을 받은 롤아웃에서 추출한 토큰 시퀀스로 구성됩니다. 이를 통해 LLM의 출력을 조정하여 더 적절한 응답을 생성할 수 있습니다. 토큰 사전은 LLM이 생성할 가능성이 낮은 토큰 시퀀스를 강조하여 탐색 공간을 효율적으로 탐색하도록 돕습니다.\n\n### [**핵심 수식**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%B5%EC%8B%AC-%EC%88%98%EC%8B%9D)\n\n1. **그룹 생성**: LLM 에이전트로부터 다양한 응답 샘플 (롤아웃)을 생성합니다. Rollouts={LLM(Inputi)∣i=1,2,…,n}Rollouts={LLM(Input*i*​)∣*i*=1,2,…,*n*} 여기서 InputiInput*i*​는 프롬프트이며, LLM(Inputi)LLM(Input*i*​)는 LLM이 생성한 응답입니다. 다양한 롤아웃을 생성하기 위해 temperature sampling 또는 top-p sampling과 같은 방법을 사용할 수 있습니다.\n\n2. **상대 평가**: 그룹 내 롤아웃들을 비교하여 상대적인 의미론적 이점을 평가합니다. Semantic Advantage=Evaluate(Rollouts)Semantic Advantage=Evaluate(Rollouts) EvaluateEvaluate 함수는 롤아웃의 품질을 평가하는 함수입니다. 이 함수는 휴리스틱 기반으로 설계될 수도 있고, 별도의 평가 모델을 사용할 수도 있습니다. 예를 들어, 수학적 추론 문제에서는 정답 여부를 평가할 수 있고, 웹 검색 작업에서는 관련성 점수를 평가할 수 있습니다.\n\n3. **지식 증류**: 상대 평가 결과를 바탕으로, LLM의 행동을 유도하는 토큰 사전을 생성합니다. Token Dictionary=Extract(Semantic Advantage)Token Dictionary=Extract(Semantic Advantage) ExtractExtract 함수는 높은 semantic advantage를 가진 롤아웃에서 유용한 토큰 시퀀스를 추출하는 함수입니다. 예를 들어, 가장 높은 점수를 받은 롤아웃에서 자주 등장하는 토큰들을 추출할 수 있습니다. 이러한 토큰들은 LLM이 올바른 방향으로 나아가도록 돕는 역할을 합니다.\n\n4. **추론 시점 적용**: 새로운 입력이 주어졌을 때, 토큰 사전을 사용하여 LLM의 출력을 조정합니다. Output=LLM(Input+Token Dictionary)Output=LLM(Input+Token Dictionary) 토큰 사전은 프롬프트에 추가되어 LLM의 출력을 유도합니다. 이때, 토큰 사전의 위치나 형식을 조정하여 성능을 최적화할 수 있습니다. 예를 들어, 토큰 사전을 프롬프트의 시작 부분에 추가하거나, 특정 토큰 사이에 삽입할 수 있습니다.\n\n5. **정책 최적화 효과**: 각 최적화 단계에서, 그룹 내 롤아웃을 비교하여 의미론적 이점을 도출하고, 이를 통해 정책 최적화 효과를 달성합니다. Optimized Policy=Optimize(Semantic Advantage)Optimized Policy=Optimize(Semantic Advantage) 이 수식은 Training-Free GRPO가 명시적인 정책 업데이트 없이도 정책 최적화 효과를 달성함을 나타냅니다. 각 단계에서 LLM은 더 나은 응답을 생성하도록 유도되며, 이는 마치 정책이 업데이트되는 것과 같은 효과를 냅니다.\n\n**코드 설명 및 개선 사항:**\n\n- `model_name = \"gpt2\"`: 더 작은 모델인 `gpt2`를 사용하여 코드 실행 가능성을 높였습니다. (GPT-2는 비교적 적은 리소스로 실행 가능)\n- `temperature=0.7`: `model.generate` 함수에 `temperature` 파라미터를 추가하여 롤아웃의 다양성을 확보했습니다.\n- `evaluate_rollouts` 함수: 정답(\"Paris\") 포함 여부를 기준으로 롤아웃을 평가하는 간단한 예시를 추가했습니다.\n- `extract_token_dictionary` 함수: 가장 높은 점수를 받은 롤아웃에서 토큰을 추출하는 예시를 추가했습니다.\n- 주석 추가: 코드의 각 부분에 대한 설명을 추가하여 이해도를 높였습니다.\n\n**주의:** 위 코드는 예시이며, 실제 사용 시에는 `evaluate_rollouts` 및 `extract_token_dictionary` 함수를 문제에 맞게 적절히 구현해야 합니다.\n\n## [**실험 설정**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%8B%A4%ED%97%98-%EC%84%A4%EC%A0%95)\n\n실험은 수학적 추론과 웹 검색 작업에서 Training-Free GRPO의 성능을 평가하기 위해 설정되었습니다. 사용된 데이터셋은 공개적으로 사용 가능한 수학 문제 데이터셋(예: MATH dataset)과 웹 검색 쿼리 데이터셋입니다. 평가 지표로는 정확도, 정밀도, 재현율 등이 사용되었습니다. 베이스라인으로는 기존의 소규모 LLM 미세 조정 방법이 사용되었습니다. 특히, few-shot learning 환경에서 Training-Free GRPO의 성능을 집중적으로 평가했습니다.\n\n### [**하이퍼파라미터 표**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%91%9C)\n\n**하이퍼파라미터값설명**학습률N/ATraining-Free 방법이므로 해당 없음배치 크기16롤아웃 생성 시 배치 크기에포크 수N/ATraining-Free 방법이므로 해당 없음최대 시퀀스 길이512LLM의 최대 입력 시퀀스 길이토큰 사전 크기100토큰 사전의 최대 토큰 수롤아웃 수5각 입력에 대해 생성하는 롤아웃 수Temperature0.7롤아웃 생성 시 temperature 값\n\n## [**실험 결과 분석**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%8B%A4%ED%97%98-%EA%B2%B0%EA%B3%BC-%EB%B6%84%EC%84%9D)\n\nTraining-Free GRPO를 적용한 결과, DeepSeek-V3.1-Terminus 모델의 성능이 크게 향상되었습니다. 특히, 수학적 추론과 웹 검색 작업에서 기존의 소규모 LLM 미세 조정 방법보다 더 나은 성능을 보였습니다. 이는 Training-Free GRPO가 LLM의 기존 지식을 효과적으로 활용하여 성능을 향상시킴을 의미합니다.\n\n### [**주요 결과 표**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%A3%BC%EC%9A%94-%EA%B2%B0%EA%B3%BC-%ED%91%9C)\n\n**작업베이스라인 정확도Training-Free GRPO 정확도성능 향상률 (%**)수학적 추론75%85%13.3%웹 검색70%82%17.1%\n\n### [**Ablation Study 분석**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#ablation-study-%EB%B6%84%EC%84%9D)\n\nAblation Study 결과, 토큰 사전의 크기와 상대 평가 방법이 성능에 큰 영향을 미치는 것으로 나타났습니다. 특히, 토큰 사전의 크기가 증가할수록 성능이 향상되었으며, 상대 평가 방법의 정교함이 성능 향상에 기여했습니다. 이는 토큰 사전이 LLM의 출력을 효과적으로 유도하고, 상대 평가 방법이 롤아웃의 품질을 정확하게 평가하는 것이 중요함을 시사합니다. 또한, 다양한 상대 평가 방법(예: 정답 여부, 관련성 점수, 사용자 피드백)을 조합하여 사용하는 것이 성능 향상에 도움이 될 수 있습니다.\n\n## [**비판적 평가**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EB%B9%84%ED%8C%90%EC%A0%81-%ED%8F%89%EA%B0%80)\n\n### [**강점**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EA%B0%95%EC%A0%90)\n\n1. **데이터 효율성**: 소수의 훈련 샘플만으로도 성능을 향상시킬 수 있는 데이터 효율적인 방법을 제시합니다.\n2. **범용성**: 다양한 도메인에서 효과적인 성능 향상을 보입니다.\n3. **과적합 방지**: 경험적 지식을 활용하여 과적합을 방지할 수 있습니다.\n4. **리소스 효율성**: 파라미터 업데이트가 필요 없으므로 연산 비용과 메모리 사용량을 줄일 수 있습니다.\n\n### [**한계점과 개선 방향**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%95%9C%EA%B3%84%EC%A0%90%EA%B3%BC-%EA%B0%9C%EC%84%A0-%EB%B0%A9%ED%96%A5)\n\n1. **상대 평가 방법의 정교함**: 상대 평가 방법이 성능에 큰 영향을 미치므로, 이를 더 정교하게 설계할 필요가 있습니다. 특히, 자동화된 평가 방법을 개발하여 평가 과정의 효율성을 높이는 것이 중요합니다.\n2. **토큰 사전의 크기**: 토큰 사전의 크기가 성능에 영향을 미치므로, 최적의 크기를 찾는 것이 중요합니다. 동적으로 토큰 사전의 크기를 조절하는 방법을 연구할 필요가 있습니다.\n3. **토큰 사전의 내용**: 토큰 사전의 내용이 편향되거나 노이즈를 포함할 경우 성능 저하를 일으킬 수 있습니다. 토큰 사전의 품질을 개선하기 위한 연구가 필요합니다.\n4. **LLM 의존성**: Training-Free GRPO는 LLM의 성능에 크게 의존합니다. LLM의 성능이 낮을 경우 효과가 제한적일 수 있습니다.\n\n### [**재현성 평가**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%9E%AC%ED%98%84%EC%84%B1-%ED%8F%89%EA%B0%80)\n\n제안된 방법론은 공개된 데이터셋과 코드로 재현이 가능하며, 실험 설정이 명확하게 설명되어 있습니다. 그러나 상대 평가 방법의 구현이 간단하게 설명되어 있어, 이를 구체화하는 것이 필요합니다. 또한, 다양한 LLM 모델과 데이터셋에 대한 실험 결과를 추가하여 일반화 성능을 검증하는 것이 중요합니다.\n\n## [**향후 연구 방향**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%ED%96%A5%ED%9B%84-%EC%97%B0%EA%B5%AC-%EB%B0%A9%ED%96%A5)\n\nTraining-Free GRPO는 다양한 도메인에서의 적용 가능성이 있습니다. 특히, 실시간으로 변화하는 환경에서의 문제 해결에 효과적일 수 있습니다. 향후 연구에서는 이러한 환경에서의 성능을 평가하고, 상대 평가 방법을 더 정교하게 설계하는 것이 필요합니다. 또한, 토큰 사전을 자동으로 생성하고 관리하는 방법을 연구하여 Training-Free GRPO의 효율성을 높이는 것이 중요합니다. 강화 학습과 결합하여 토큰 사전을 학습하는 방법도 연구할 가치가 있습니다.\n\n## [**실무 적용 가이드**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%8B%A4%EB%AC%B4-%EC%A0%81%EC%9A%A9-%EA%B0%80%EC%9D%B4%EB%93%9C)\n\nTraining-Free GRPO를 실무에 적용할 때는 다음과 같은 점을 고려해야 합니다:\n\n1. **토큰 사전의 크기**: 최적의 토큰 사전 크기를 찾는 것이 중요합니다. 이는 실험을 통해 조정할 수 있습니다. 다양한 크기의 토큰 사전을 사용하여 성능 변화를 관찰하고, 적절한 크기를 선택해야 합니다.\n2. **상대 평가 방법**: 상대 평가 방법을 정교하게 설계하여 성능을 최적화해야 합니다. 문제의 특성에 맞는 평가 지표를 선택하고, 필요에 따라 여러 평가 지표를 조합하여 사용해야 합니다.\n3. **데이터 효율성**: 소수의 훈련 샘플로도 성능을 향상시킬 수 있으므로, 데이터 수집 비용을 절감할 수 있습니다. 기존에 보유하고 있는 데이터를 최대한 활용하고, 필요한 경우 소량의 데이터를 추가적으로 수집하는 전략을 세워야 합니다.\n4. **모델 선택**: Training-Free GRPO는 LLM의 성능에 의존적이므로, 적절한 LLM 모델을 선택하는 것이 중요합니다. 문제의 복잡도와 필요한 지식 수준을 고려하여 LLM 모델을 선택해야 합니다.\n\n## [**결론**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EA%B2%B0%EB%A1%A0)\n\nTraining-Free GRPO는 매개변수 업데이트 없이도 LLM의 성능을 향상시킬 수 있는 새로운 방법론입니다. 이 방법론은 경험적 지식을 토큰 사전으로 학습하여 LLM의 출력 분포를 조정하며, 데이터 부족 문제를 해결하고, 과적합을 방지할 수 있습니다. 다양한 도메인에서의 성능 향상을 통해 LLM 에이전트의 활용 가능성을 넓히고, 데이터 효율적인 방식으로 성능을 향상시킬 수 있는 잠재력을 제시합니다. 특히, few-shot learning 환경에서 강력한 성능을 보이며, 리소스 제약이 있는 환경에서 유용하게 활용될 수 있습니다.\n\n## [**참고 자료**](https://suanlab.com/blog/20260101-paper-2510-08191-training-free-group-relative-p/#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C)\n\n- [논문 링크](https://arxiv.org/abs/2510.08191)\n- [코드 저장소](https://github.com/example-repo/Training-Free-GRPO)\n- 관련 자료: [DeepSeek-V3.1-Terminus 모델](https://example.com/deepseek-v3.1-terminus)\n- [LoRA(Low-Rank Adaptation) 논문](https://arxiv.org/abs/2106.09698)\n- [DPO(Direct Preference Optimization) 논문](https://arxiv.org/abs/2305.18290)\n- [AutoPrompt 논문](https://arxiv.org/abs/2003.10581)",
    "id": "8679d703-1cf9-4032-bb96-a091d081cc60",
    "createdAt": "2026-02-17T12:57:00.096Z",
    "updatedAt": "2026-02-17T12:58:07.254Z"
  },
  {
    "citeKey": "github2026",
    "url": "https://github.com/p-e-w/heretic",
    "type": "blog",
    "title": "GitHub - p-e-w/heretic: Fully automatic censorship removal for language models",
    "authors": [],
    "date": "",
    "description": "Fully automatic censorship removal for language models - p-e-w/heretic",
    "thumbnail": "https://opengraph.githubassets.com/dd65be10f860f311fd4d6f55e0ac2b9823c6002c86aebaf197f8cdf09e34ad91/p-e-w/heretic",
    "siteName": "GitHub",
    "tags": [],
    "collections": [],
    "notes": "",
    "id": "d9ab135b-062d-436c-8f1e-053c5d2ec76c",
    "createdAt": "2026-02-17T13:07:48.034Z",
    "updatedAt": "2026-02-17T13:07:48.034Z"
  },
  {
    "citeKey": "github2026a",
    "url": "https://github.com/ComposioHQ/composio",
    "type": "blog",
    "title": "GitHub - ComposioHQ/composio: Composio equips your AI agents & LLMs with 100+ high-quality integrations via function calling",
    "authors": [],
    "date": "",
    "description": "Composio equips your AI agents & LLMs with 100+ high-quality integrations via function calling - ComposioHQ/composio",
    "thumbnail": "https://opengraph.githubassets.com/800e1758f9f0cf3e976fd1fc1b8279d289a5b5fd0671659d3800bbfb75cdc4a4/ComposioHQ/composio",
    "siteName": "GitHub",
    "tags": [],
    "collections": [],
    "notes": "",
    "id": "18ac64c6-c275-4feb-8579-12134350ea2a",
    "createdAt": "2026-02-17T13:07:58.458Z",
    "updatedAt": "2026-02-17T13:07:58.458Z"
  },
  {
    "citeKey": "compound2026",
    "url": "https://limc.dev/log/article/dev/ai/ar-da-02-en-compound-engineering-introducing-it-using-codex-skills/",
    "type": "blog",
    "title": "\"Compound Engineering\" - Introducing it using Codex Skills - SnaqSh0t",
    "authors": [],
    "date": "",
    "description": "기록 남기는 곳",
    "thumbnail": "https://limc.dev/assets/og.png",
    "siteName": "SnaqSh0t",
    "tags": [],
    "collections": [],
    "notes": "# **\"Compound Engineering\" - Introducing it using Codex Skills**\n\n**AR-DA-02-EN**calendar_today2026-02-17 00:39#AGENTS #CODEX #AI\n\n## **Overview**\n\n- This note keeps the original `ce-*` files from `~/.agents/skills` as shared references and adds what each file solves.\n- The reason this post is long is that I kept the original skill contents instead of summarizing them away.\n- Start from sections 1 to 4, then refer to section 5 onward when needed for exact file details.\n\n## **1. Rules in your head evaporate**\n\nAt first, everyone says the same things.\n\n- \"We should always create a plan before starting\"\n- \"Reviews should always happen\"\n- \"The same bug should never come back\"\n\nThen timelines get tight, requests pile up, and parallel work starts piling up, and those rules blur quickly.\n\nAnd only when the cost returns, it starts to come back to you.\n\n- Scope changes, but implementation keeps going as before.\n- We ran tests, but never documented what was actually verified.\n- We reviewed, but never agreed on what was the highest-risk issue.\n- The bug is fixed, then appears again in almost the same shape the next week.\n\nThat is why I decided not to keep these as abstract sentences.\n\nI enforced them by turning AGENTS-like rules into Skills files.\n\n## **2. Turn rules into skill files**\n\nTo use this consistently across every project in my environment, I created these skills under `~/.agents/skills`.\n\n*I’ll describe this without leaking absolute paths, using home-directory notation.*\n\nThe idea is simple.\n\n- The loop is fixed to `Plan → Work → Review → Compound`.\n- It is based on the notes from [AR-DA-01 Compound Engineering Notes](https://limc.dev/log/article/dev/ai/ar-da-01-compound-engineering-%EB%A5%BC-%EC%9D%BD%EA%B3%A0/).\n- Each stage has explicit deliverables in files.\n- Stage transitions are blocked by explicit approval gates.\n  - I previously tried to enforce this only with [AGENTS.md](http://agents.md/), but it kept being bypassed and phases were mixed. ...\n\n## **3. Current skill composition**\n\nBelow is the skill-only directory structure used for the Compound-Engineering workflow.\n\ntextcontent_copy\n\n```\n~/.agents/skills/├── README.md├── ce-router/├── ce-plan/├── ce-review/└── ce-compound/\n```\n\nThe effect is immediate.\n\n- On a brand-new project, the same prompt is always used.\n- It is explicit when we should stop and wait for approval.\n\n## **4. [README.md](http://readme.md/): a map for this workflow**\n\n`README.md` gives context and minimal orientation for the toolkit.\n\nmdcontent_copy\n\n```\n## Skill set for this kit- ce-router (gate/routing): implicit allowed (auto-intervention where possible)- ce-plan (PLAN deliverables)- ce-work (WORK execution + mandatory Work Log)- ce-review (review + P1/P2/P3)- ce-compound (at least one carry-over artifact required)\n```\n\nThe effect is simple.\n\n- It saves time searching for \"what skill exists\".\n- It keeps the loop components visible in one place.\n- It naturally nudges people into moving stages in order.\n- The files are mainly for humans, not for agents.\n\n## **5. ce-router: entrypoint (routing + approval gate)**\n\nIn rapid, parallel-driven work, uncertainty about current stage is common.\n\nWhen \"what step are we in\" becomes unclear, the next stage usually unravels.\n\n`ce-router` exists to remove that ambiguity.\n\nmdcontent_copy\n\n```\n---name: ce-routerdescription: |  If the user request is unclear about which stage it belongs to among PLAN/WORK/REVIEW/COMPOUND,  or asks for implementation/fixes/tests/deployments without approved PLAN, this skill enforces  the Compound Engineering loop by forcing stage routing and explicit handoff.  For requests like \"implement\", \"fix\", \"refactor\", \"run tests\", \"deploy\",  it explicitly tells the assistant the current phase, approval state, and next skill to call.---## Core Rules- All responses are written in Korean.- Code comments are always written in Korean.- Single-person development is assumed; final decisions belong to the user.- AI leads planning → implementation → testing → documentation, and stage transitions happen only after explicit user approval.  - Exception: when the user explicitly requests uninterrupted execution.## Routing Objective- Classify the request into exactly one stage.  - PLAN / WORK / REVIEW / COMPOUND- Ask only 1~3 high-impact questions if more information is needed.## Approval Gate (Mandatory)- Do not move to WORK without an approved PLAN.- Even with an approved PLAN, if stage transition is not approved:  - Stop and ask one sentence: \"Can I continue to the next stage?\"- If the user requests uninterrupted execution:  - Continue, except when Plan Drift (scope/policy/invariant changes) occurs; then stop and re-confirm with 1~3 questions.## Output Format (Mandatory)1) Current-state summary (3~6 lines)   - Interpreted objective   - Current stage (estimated or confirmed)   - Approval/delegation status2) One next stage and reason3) Questions 1~3 (only if needed)4) Input block template for the next stage   - Copy-paste ready5) Final line: `CE ROUTING COMPLETE`\n```\n\n### **5.1. Effect from this skill**\n\n- It blocks accidental transitions from PLAN to WORK.\n- It turns phase change from \"implicit mood\" into \"explicit approval\".\n- It caps follow-up questions to 1\\~3 only.\n\n## **6. ce-plan: fixing plan as an explicit artifact**\n\nThe purpose of Plan is not a giant design doc.\n\nIt is the minimum guardrail that prevents Work from drifting.\n\nmdcontent_copy\n\n```\n---name: ce-plandescription: |  Compose the PLAN stage outputs for Compound Engineering.  Without implementing or running tests, write goals, scope, DoD, defect mapping,  invariants, policy, checklist, test plan, and risk controls in a high-quality plan.  Ask up to 3 questions only when uncertain.---## Mode Lock- PLAN-only.- No implementation, code edits, or test execution.- Do not make unsupported assumptions; if evidence is missing, label it as assumption and escalate for approval.## User Input (required)Users provide as much as available from the format below. If missing, ask 1~3 questions.- Project:- Stack/runtime:- Work ID:- Work title:- Current stage: PLAN- SoT/reference files (optional): (path list)- Target requirements/defects: (list)- Invariants/policies: (list)- Scope include / exclude- Non-negotiable invariants: (list)- (Optional) Code hints: file paths/modules/endpoints## Quality Standards (Checklist)- First Principles: refine requirements into atomic units- Invariants First: lock invariant protection as strategy core- Failure-Oriented: design with failure/exception/boundary first- Minimal Complexity: start from the simplest approach- Tight Loop: enforce stage-by-stage verification loops- Observability: include logging/metrics/error-message improvement points## Evidence Rule (Mandatory)- If possible, attach evidence as \"file path + line range\" for each claim.- If evidence cannot be verified yet, write:  - (1) Unverified evidence  - (2) Candidate files to verify  - (3) Commands/methods to verify  in the PLAN document.## Output Format (Mandatory)Must include the following 13 sections in order.1) Goal / scope / out-of-scope2) Defect or requirement matrix3) Invariant protection strategy4) Policy decision table5) Completion criteria (DoD, measurable)6) Candidate changed files + rationale7) Stagewise Work checklist (W1~Wn, validation command per stage)8) Test plan (Red → Green split, happy/failure/boundary)9) Risks/regressions (P1/P2/P3) + response   - P1: data/security/crash/critical regression   - P2: durability / major quality / high usability impact   - P3: documentation / cleanup / improvements10) Plan Drift rules (stop/approval conditions)11) Review priority criteria12) Compound candidates (minimum 1)13) User approval points (1~3)### Top summary (required)- Add a top-of-document summary within 5 lines.### Bottom sentinel (required)- Final line: `PLAN COMPLETE — REQUEST WORK APPROVAL`\n```\n\n### **6.1. Effects of this skill**\n\n- Goals/scope/constraints are fixed as written agreement.\n- DoD makes \"done\" measurable and reviewable.\n- It forces failure/exception/boundary thinking first, reducing late-stage debugging.\n\n## **7. ce-work: combining execution, verification, and logs**\n\nIn Work, two mistakes happen again and again.\n\n- Continuing beyond plan without noticing scope drift.\n- Running checks but leaving no evidence of what was verified.\n\n`ce-work` focuses on both at once.\n\nmdcontent_copy\n\n```\n---name: ce-workdescription: |  Execute the WORK stage based on an approved PLAN.  Force a cycle of one checklist item at a time, verify what was done,  and enforce keeping a Work Log.  Stop and request approval immediately when Plan Drift is detected.---## Preconditions (Gate)- Do not execute WORK without approved PLAN.  - Instead, ask 1~3 missing questions or guide to $ce-plan.- Continuous execution is only allowed when user explicitly says no intermediate confirmation.  - Stop immediately and re-confirm approval if Plan Drift occurs.## Basic Rules- All responses are in Korean.- Code comments are always in Korean.- No work outside the approved PLAN scope.- Enforce meaningful unit commits (commit message must include what and why).## User Input- Work ID:- Work title:- Current stage: WORK- Approved PLAN: (paste document path or summary)- (Optional) Branch/PR strategy:- (AGENTS.md at repo root) run/test commands:## Execution Unit (Mandatory)1) Select one checklist item2) Implement within PLAN scope3) Verify (tests/lint/type-check/manual scenarios where possible)4) Commit with what/why5) Update Work Log## Plan Drift Handling (Mandatory)- Pause immediately if any of these occur:  - Scope inversion (include/exclude flipped)  - Possible fixed-policy violation  - Risk of breaking invariants  - Need to redefine DoD- On pause, provide:  - Before/after state  - Why the change happened  - Impact (P1/P2/P3)  - 1~2 alternatives  - One-line re-approval request## Work Log (Mandatory)- At WORK completion, leave at least one:  - docs/plans/<work-id>-worklog.md creation  - or add a `## Work Log` section at the end of docs/plans/<work-id>-plan.md### Work Log Minimum Items- Progress summary (3~7 lines)- Final changed file list- Checklist completion status ([x])- Commands run and results (only what ran)- Commit hash or PR number/link- Final 3 lines at work end:  - Completed checklist items in this WORK:  - Validations passed in this state:  - Risk points to watch in Review (if any):## Recommended Output Format (Operational)- Checklist items completed in this turn- Changed files + core reason- Validation commands/results- Work Log update summary- Next checklist proposal- Final line: `WORK STATUS SHARED — REQUEST NEXT STAGE APPROVAL`\n```\n\n### **7.1. Effects of this skill**\n\n- Work is broken into small completion slices instead of a single large pass.\n- Plan Drift is blocked early, reducing late-stage scope inflation.\n- Work Log creation is enforced and reduces handoff cost.\n\n## **8. ce-review: turning work into alignment**\n\nActual review is where we decide whether a loop outcome is ready.\n\nThe key is forcing prioritization.\n\n- P1: must-fix now.\n- P2: should be fixed if possible in this cycle.\n- P3: can be deferred.\n\nmdcontent_copy\n\n```\n---name: ce-reviewdescription: |  Run the REVIEW stage after WORK.  Start with a self PR review, and prioritize checks in order: bugs/edges, risks,  regressions, then readability/maintainability.  Classify issues as P1/P2/P3 and summarize so the user can choose what to fix next.---## User Input- Work ID:- Work title:- Current stage: REVIEW- Approved PLAN (path or summary):- Change summary (if available):- Commit/PR identifier (if available):- Validation results (if available):## Review Priority (Mandatory)1) Bug likelihood / edge cases2) Risks (security, data loss, performance, operations)3) Regression risk (impact on existing behavior)4) Readability / maintainability## Issue Classification (Mandatory)- P1: Must fix now (data/security/crash/critical regression)- P2: Fix if possible this cycle (usability/durability/important quality)- P3: Documentation/cleanup/improvements (deferable)## Evidence Rule (Mandatory)- Use file path + line range as evidence whenever possible.- If verification is not complete, mark as \"evidence not verified\" and list candidate files/lines to check.## Output Format (Mandatory)1) Change summary (3~7 lines)2) DoD completion check (by each PLAN DoD item)3) P1 / P2 / P3 issue list   - For each: symptom / impact / reproduction or condition / evidence (file:line) / recommended fix4) Additional test/observability points (happy/failure/boundary)5) User decisions required (1~3)6) Final line: `REVIEW COMPLETE — REQUEST APPROVAL FOR NEXT STAGE (COMPOUND OR ADDITIONAL WORK)`\n```\n\n### **8.1. Effects of this skill**\n\n- Review quality stays stable by locking priorities to bug/risk/regression/readability.\n- P1/P2/P3 makes decision-making explicit.\n- Evidence includes file/line references, reducing rework.\n\n## **9. ce-compound: one more pass for reusability**\n\nWithout this final stage, loops often stop at \"we did the work\".\n\n`ce-compound` pushes it one step further.\n\nmdcontent_copy\n\n```\n---name: ce-compounddescription: |  Run the COMPOUND stage.  Every work cycle must leave at least one carry-over asset for the next cycle,  such as tests, docs, guardrails, rules, or observability improvements.  Answer the question: if this issue happens again, will tests/rules catch it automatically?  If not, leave at least one artifact that makes future discovery easier.---## Input- Work ID:- Work title:- Current stage: COMPOUND- Final change summary:- Proposed carry-over artifact candidates (if any):## Mandatory Rules- Propose at least one carry-over artifact (preferably immediately actionable).- Answer these questions:  - If this issue happens again, will tests/rules/guards catch it automatically?  - If not, what should be left so it is easier to find next time?## Recommended Artifact Paths- docs/solutions/<slug>.md (recommended)- Or:  - Add regression tests  - Strengthen AGENTS.md rules (short, strict)  - Update review checklist  - Improve observability (logs/error messages/tracing points)## Output Format (Mandatory)1) 1~3 candidate artifacts (priority order)2) At least one should be immediately actionable   - propose file path + doc/test skeleton3) Expected effect (why next work is easier)4) User approval points (1~3)5) Final line: `COMPOUND COMPLETE — REQUEST REPEAT APPROVAL`\n```\n\n### **9.1. Effects of this skill**\n\n- It forces explicit check: will the issue be automatically caught next time?\n- It requires at least one reusable carryover item (test/rules/docs/observability).\n- Even one persisted artifact lowers the cost of repeated mistakes.\n\n## **10. Conclusion**\n\nThe reason I keep this structure is simple.\n\n- The loop protects people by encoding the process in files, not vibes.\n- It preserves evidence, not just execution.\n- Every cycle leaves the workspace easier to continue from.\n\nPlan → Work → Review → Compound is not just a set of names.\n\nIt is a method for locking approval, verification, and anti-regression into files.",
    "id": "2e5d0704-e8b1-435c-aa9e-a021621f38f5",
    "createdAt": "2026-02-17T13:08:33.714Z",
    "updatedAt": "2026-02-17T13:08:50.410Z"
  },
  {
    "citeKey": "github2026b",
    "url": "https://github.com/maxritter/claude-pilot",
    "type": "blog",
    "title": "GitHub - maxritter/claude-pilot: Claude Code is powerful. Pilot makes it reliable. Start a task, grab a coffee, come back to production-grade code. Tests enforced. Context preserved. Quality automated.",
    "authors": [],
    "date": "",
    "description": "Claude Code is powerful. Pilot makes it reliable. Start a task, grab a coffee, come back to production-grade code. Tests enforced. Context preserved. Quality automated. - maxritter/claude-pilot",
    "thumbnail": "https://repository-images.githubusercontent.com/1078701786/7d7aea0c-a0be-4d67-a26a-2d31bb680c4a",
    "siteName": "GitHub",
    "tags": [],
    "collections": [],
    "notes": "",
    "id": "e70c1a1a-9a8d-4e8b-8c12-5e3f3424eafb",
    "createdAt": "2026-02-17T13:09:54.527Z",
    "updatedAt": "2026-02-17T13:09:54.527Z"
  },
  {
    "citeKey": "github2026c",
    "url": "https://github.com/brendanhogan/hermitclaw/",
    "type": "blog",
    "title": "GitHub - brendanhogan/hermitclaw",
    "authors": [],
    "date": "",
    "description": "Contribute to brendanhogan/hermitclaw development by creating an account on GitHub.",
    "thumbnail": "https://opengraph.githubassets.com/3873e60cbce772c3f4879a6d33ed848f334adf28c3d2975389edda0a4be22eca/brendanhogan/hermitclaw",
    "siteName": "GitHub",
    "tags": [],
    "collections": [],
    "notes": "HermitClaw는 데스크탑의 특정 폴더만 접근할 수 있도록 제한된 AI 에이전트입니다.\\\n해당 폴더 안에서만 작동하며, 파일을 넣어주면 코드를 짜거나 웹 검색, 글쓰기, 리서치 등을 스스로 수행합니다.\\\n몇 초마다 자체적인 '기분'이나 '계획'에 따라 짧게 사고하고 행동하며, 기억은 Smallville 논문(Generative Agents, Park et al. 2023)을 참고해 시간, 중요도와 함께 저장됩니다.\\\n중요한 생각이 일정량 쌓이면 잠시 멈춰 자신이 한 생각을 되돌아보고 요약까지 수행합니다.",
    "id": "f2087e4a-9dc5-4d6e-bef7-0a47c835e104",
    "createdAt": "2026-02-17T13:10:43.201Z",
    "updatedAt": "2026-02-17T13:10:54.321Z"
  },
  {
    "citeKey": "yuchen2026",
    "url": "https://arxiv.org/abs/2602.12394",
    "type": "academic",
    "title": "Synthetic Interaction Data for Scalable Personalization in Large Language Models",
    "authors": [
      "Ma",
      "Yuchen",
      "Huang",
      "Yue",
      "Wang",
      "Wenjie",
      "Luo",
      "Xiaonan",
      "Zhang",
      "Xiangliang",
      "Feuerriegel",
      "Stefan"
    ],
    "date": "2026/02/12",
    "description": "Personalized prompting offers large opportunities for deploying large language models (LLMs) to diverse users, yet existing prompt optimization methods primarily focus on task-level optimization while largely overlooking user-specific preferences and latent constraints of individual users. This gap is primarily due to (i) the absence of high-quality, privacy-sensitive data that capture personalized user-LLM interactions at scale, and (ii) the lack of robust reward signals for individual preferences. To overcome existing data limitations, we introduce a high-fidelity synthetic data generation framework called PersonaGym. Unlike prior work that treats personalization as static persona-preference pairs, PersonaGym models a dynamic preference process via an agentic LLM system to simulate realistic preference behaviors and semantic-aware noise in order to generate personalized multi-turn interaction trajectories. Using PersonaGym, we release PersonaAtlas, a large-scale, high-quality, and diverse synthetic dataset of high-fidelity multi-turn personalized interaction trajectories that closely mirror real-world preference expression and noise patterns. We further propose Personalized Prompt Optimization (PPOpt), a scalable and model-agnostic framework that optimizes user prompts based on interaction histories without modifying the deployed LLM. PPOpt adopts a reason-then-optimize paradigm that infers an explicit user profile and conditions prompt rewriting on the user profile to avoid reward hacking. Our training procedure for PPOpt integrates a cold-start supervised prior with outcome-driven multi-objective reinforcement learning. We present extensive experiments to demonstrate consistent improvements over state-of-the-art baselines in terms of task performance, personalization quality, and robustness to noisy as well as to sparse preference signals.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [],
    "collections": [],
    "notes": "PersonaGym",
    "id": "4bd931e6-e1f3-4cd0-a6b1-326dbcec1b8b",
    "createdAt": "2026-02-17T13:11:50.665Z",
    "updatedAt": "2026-02-17T13:11:58.565Z"
  },
  {
    "citeKey": "ruihan2026",
    "url": "https://arxiv.org/abs/2602.12662",
    "type": "academic",
    "title": "Think Fast and Slow: Step-Level Cognitive Depth Adaptation for LLM Agents",
    "authors": [
      "Yang",
      "Ruihan",
      "Ye",
      "Fanghua",
      "We",
      "Xiang",
      "Zhao",
      "Ruoqing",
      "Luo",
      "Kang",
      "Xu",
      "Xinbo",
      "Zhao",
      "Bo",
      "Ma",
      "Ruotian",
      "Wang",
      "Shanyi",
      "Tu",
      "Zhaopeng",
      "Li",
      "Xiaolong",
      "Yang",
      "Deqing",
      "Linus"
    ],
    "date": "2026/02/13",
    "description": "Large language models (LLMs) are increasingly deployed as autonomous agents for multi-turn decision-making tasks. However, current agents typically rely on fixed cognitive patterns: non-thinking models generate immediate responses, while thinking models engage in deep reasoning uniformly. This rigidity is inefficient for long-horizon tasks, where cognitive demands vary significantly from step to step, with some requiring strategic planning and others only routine execution. In this paper, we introduce CogRouter, a framework that trains agents to dynamically adapt cognitive depth at each step. Grounded in ACT-R theory, we design four hierarchical cognitive levels ranging from instinctive responses to strategic planning. Our two-stage training approach includes Cognition-aware Supervised Fine-tuning (CoSFT) to instill stable level-specific patterns, and Cognition-aware Policy Optimization (CoPO) for step-level credit assignment via confidence-aware advantage reweighting. The key insight is that appropriate cognitive depth should maximize the confidence of the resulting action. Experiments on ALFWorld and ScienceWorld demonstrate that CogRouter achieves state-of-the-art performance with superior efficiency. With Qwen2.5-7B, it reaches an 82.3% success rate, outperforming GPT-4o (+40.3%), OpenAI-o3 (+18.3%), and GRPO (+14.0%), while using 62% fewer tokens.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [
      "Agent",
      "Level thinking"
    ],
    "collections": [],
    "notes": "이 논문은 **대형 언어 모델(LLM) 에이전트의 사고 방식 적응**을 다루고 있습니다. 핵심 요점은 다음과 같습니다:\n\n### 문제 인식\n\n- 기존 LLM 에이전트는 **고정된 사고 패턴**을 사용합니다:\n\n  - \"즉각 반응형(non-thinking)\" 모델은 모든 단계에서 단순 반응만.\n\n  - \"깊은 사고(thinking)\" 모델은 모든 단계에서 복잡한 추론만.\n\n- 이런 방식은 **비효율적**입니다. 단순한 단계에도 불필요하게 많은 토큰을 쓰거나, 복잡한 단계에서 단순 반응으로 실패합니다.\n\n### 제안된 해결책: **COGROUTER**\n\n- **ACT-R 인지 이론**을 기반으로, 단계별로 사고 깊이를 조절하는 프레임워크.\n\n- 네 가지 인지 수준(L1–L4)을 정의:\n\n  - L1: 직관적 반응\n\n  - L2: 상황 인식\n\n  - L3: 경험 반영\n\n  - L4: 전략적 계획\n\n- **2단계 학습 과정**:\n\n  1. **COSFT (Cognition-aware Supervised Fine-tuning)**: 각 수준별 사고 패턴을 안정적으로 학습.\n\n  2. **COPO (Cognition-aware Policy Optimization)**: 강화학습을 통해 단계별로 적절한 사고 깊이를 선택. 핵심은 **행동 예측의 확신(confidence**)을 기준으로 보상 재분배.\n\n### 주요 성과\n\n- **ALFWorld**와 **ScienceWorld** 벤치마크에서 평가.\n\n- **Qwen2.5-7B** 모델 기반으로:\n\n  - 평균 성공률 **82.3%** 달성.\n\n  - GPT-4o 대비 +40.3%, OpenAI-o3 대비 +18.3%, GRPO 대비 +14.0% 향상.\n\n  - 토큰 사용량은 기존 대비 **62% 절감**.\n\n- COPO는 기존 RL 방법들이 빠지기 쉬운 **“깊은 사고(L4)로의 붕괴**”를 방지하고, 단계별로 적절한 사고 깊이를 배분.\n\n### 기여 요약\n\n1. LLM 에이전트의 **인지 경직성(cognitive rigidity)** 문제를 정의.\n\n2. **COGROUTER** 프레임워크 제안: 다단계 인지 수준 + 2단계 학습.\n\n3. **COPO 알고리즘** 도입: 확신 기반 보상 재분배로 단계별 사고 깊이 최적화.\n\n즉, 이 논문은 **“모든 단계에서 똑같이 깊게 생각하는 대신, 상황에 맞게 사고 깊이를 조절하는 LLM 에이전트**”를 만드는 방법을 제시하고, 실제로 더 높은 성능과 효율성을 입증했습니다.",
    "id": "005fafba-f4c0-4c87-a920-5d0701ea9737",
    "createdAt": "2026-02-17T13:13:30.854Z",
    "updatedAt": "2026-02-17T13:15:25.792Z"
  },
  {
    "citeKey": "junjie2026",
    "url": "https://arxiv.org/abs/2602.12852",
    "type": "academic",
    "title": "WebClipper: Efficient Evolution of Web Agents with Graph-based Trajectory Pruning",
    "authors": [
      "Wang",
      "Junjie",
      "Xie",
      "Zequn",
      "Yang",
      "Dan",
      "Feng",
      "Jie",
      "Shen",
      "Yue",
      "Sun",
      "Duolin",
      "Long",
      "Meixiu",
      "Jiao",
      "Yihan",
      "Tan",
      "Zhehao",
      "Wang",
      "Jian",
      "Wei",
      "Peng",
      "Gu",
      "Jinjie"
    ],
    "date": "2026/02/13",
    "description": "Deep Research systems based on web agents have shown strong potential in solving complex information-seeking tasks, yet their search efficiency remains underexplored. We observe that many state-of-the-art open-source web agents rely on long tool-call trajectories with cyclic reasoning loops and exploration of unproductive branches. To address this, we propose WebClipper, a framework that compresses web agent trajectories via graph-based pruning. Concretely, we model the agent's search process as a state graph and cast trajectory optimization as a minimum-necessary Directed Acyclic Graph (DAG) mining problem, yielding pruned trajectories that preserve essential reasoning while eliminating redundant steps. Continued training on these refined trajectories enables the agent to evolve toward more efficient search patterns and reduces tool-call rounds by about 20% while improving accuracy. Furthermore, we introduce a new metric called F-AE Score to measure the model's overall performance in balancing accuracy and efficiency. Experiments demonstrate that WebClipper compresses tool-call rounds under excellent performance, providing practical insight into balancing effectiveness and efficiency in web agent design.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [
      "Agent",
      "Web",
      "WebAgent"
    ],
    "collections": [],
    "notes": "",
    "id": "3bf958c8-323e-42d3-b492-4988ef490dda",
    "createdAt": "2026-02-17T13:16:15.078Z",
    "updatedAt": "2026-02-17T13:16:15.078Z"
  },
  {
    "citeKey": "futing2026",
    "url": "https://arxiv.org/abs/2602.11748",
    "type": "academic",
    "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
    "authors": [
      "Wang",
      "Futing",
      "Yan",
      "Jianhao",
      "Luo",
      "Yun",
      "Cui",
      "Ganqu",
      "Wang",
      "Zhi",
      "Qu",
      "Xiaoye",
      "Zhang",
      "Yue",
      "Cheng",
      "Yu",
      "Lin",
      "Tao"
    ],
    "date": "2026/02/12",
    "description": "Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.\n  Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.\n  To bridge this gap, we propose Length-Incentivized Exploration(\\method).\n  This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.\n  Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration.\n  As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks.",
    "thumbnail": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
    "siteName": "arXiv.org",
    "tags": [],
    "collections": [
      "Agent"
    ],
    "notes": "1\\. test-time scaling에서 중요한 능력을 in-context exploration으로 정의함. 한 번의 연속 컨텍스트 안에서 가설을 여러 개 생성하고, 검증하고, 수정하면서 상태를 넓게 탐색하는 능력임. \n\n2\\. 이 능력을 state coverage 관점으로 분석하고, 긴 reasoning trajectory가 더 넓은 state coverage를 만들지만 autoregressive 샘플링에서는 긴 시퀀스가 나올 확률이 지수적으로 줄어 “shallow exploration trap”에 빠진다고 정식화함. \n\n3\\. 표준 RLVR/GRPO류 학습은 어느 정도 길이를 늘리는 경향이 있어도, 길이를 “정보 탐색”이 아니라 “반복/군더더기”로 채워서 state density가 떨어지는 문제가 생긴다고 보고, 길이만 늘리면 해결되지 않는다고 주장함. \n\n4\\. 해결책으로 LIE(length-incentivized exploration)라는 reward shaping 레시피를 제안함. 핵심은 2단 구성임: (a) 길이 기반 보상으로 탐색 용량(더 긴 trajectory)을 강제로 확보하고, (b) redundancy penalty로 반복을 억제해 같은 길이에서 더 많은 “서로 다른 상태”를 밟게 만들어 실제 state coverage를 키움. \n\n5\\. 길이 보상은 샘플별로 현재 정책이 원래 내는 길이를 기준으로 target length를 두고, 최소한 그보다 더 길게(ΔL만큼) 생성하도록 유도하는 형태로 설명됨. 이때 길이만 인센티브하면 distinct ratio가 떨어지고 반복이 늘어나는 부작용이 실험에서 확인됨. \n\n6\\. redundancy penalty는 “긴데 실질 상태를 안 늘리는 출력”을 깎아, 길이 보상이 유도한 추가 토큰이 반복 채우기가 아니라 새로운 가설/검증/수정 같은 탐색 행동으로 쓰이게 만드는 역할로 제시됨. \n\n7\\. 실험은 Qwen3, Llama 계열에서 수행되며, LIE가 in-domain reasoning 태스크 평균 +4.4%, out-of-domain 벤치 평균 +2.7% 향상을 보고함.",
    "id": "e05bfd84-50c9-41af-b0e1-b35fb4505588",
    "createdAt": "2026-02-17T13:23:22.598Z",
    "updatedAt": "2026-02-18T06:09:57.490Z"
  }
]